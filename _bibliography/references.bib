
@book{Ahrens_2017_How_to_take_smart_notes_one_simple_technique_to_boost_writing_learning_and_thinking_for_students_academics_and_nonfiction_book_writers,
  address = {{North Charleston, SC}},
  title = {How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking: For Students, Academics and Nonfiction Book Writers},
  isbn = {978-1-5428-6650-7},
  shorttitle = {How to Take Smart Notes},
  language = {eng},
  publisher = {{CreateSpace}},
  author = {Ahrens, S{\"o}nke},
  year = {2017}
}


@book{Kahneman_2013_Thinking_fast_and_slow,
  address = {{New York}},
  edition = {1st pbk. ed},
  title = {Thinking, Fast and Slow},
  isbn = {978-0-374-53355-7},
  lccn = {BF441 .K238 2013},
  abstract = {In this work the author, a recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology that challenged the rational model of judgment and decision making, has brought together his many years of research and thinking in one book. He explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. He exposes the extraordinary capabilities, and also the faults and biases, of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. He reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives, and how we can use different techniques to guard against the mental glitches that often get us into trouble. This author's work has transformed cognitive psychology and launched the new fields of behavioral economics and happiness studies. In this book, he takes us on a tour of the mind and explains the two systems that drive the way we think and the way we make choices},
  publisher = {{Farrar, Straus and Giroux}},
  author = {Kahneman, Daniel},
  year = {2013}
}



@article{Crane_2018_Questionable_Answers_in_Question_Answering_Research_Reproducibility_and_Variability_of_Published_Results,
  title = {Questionable {{Answers}} in {{Question Answering Research}}: {{Reproducibility}} and {{Variability}} of {{Published Results}}},
  volume = {6},
  shorttitle = {Questionable {{Answers}} in {{Question Answering Research}}},
  language = {en-us},
  journal = {Transactions of the Association for Computational Linguistics},
  doi = {10.1162/tacl_a_00018},
  url = {https://aclweb.org/anthology/papers/Q/Q18/Q18-1018/},
  author = {Crane, Matt},
  year = {2018},
  pages = {241-252}
}


@inproceedings{EscartinReijersEtAl_2017_Ethical_Considerations_in_NLP_Shared_Tasks,
  title = {Ethical {{Considerations}} in {{NLP Shared Tasks}}},
  language = {en-us},
  booktitle = {Proceedings of the {{First ACL Workshop}} on {{Ethics}} in {{Natural Language Processing}}},
  doi = {10.18653/v1/W17-1608},
  url = {https://aclweb.org/anthology/papers/W/W17/W17-1608/},
  author = {Escart{\'i}n, Carla Parra and Reijers, Wessel and Lynn, Teresa and Moorkens, Joss and Way, Andy and Liu, Chao-Hong},
  month = apr,
  year = {2017},
  pages = {66-73}
}


@inproceedings{SugawaraStenetorpEtAl_2020_Assessing_Benchmarking_Capacity_of_Machine_Reading_Comprehension_Datasets,
  title = {Assessing the {{Benchmarking Capacity}} of {{Machine Reading Comprehension Datasets}}},
  booktitle = {{{AAAI}}},
  author = {Sugawara, Saku and Stenetorp, Pontus and Inui, Kentaro and Aizawa, Akiko},
  year = {2020},
  url = {http://arxiv.org/abs/1911.09241},
  archivePrefix = {arXiv}
}


@inproceedings{JiaLiang_2017_Adversarial_Examples_for_Evaluating_Reading_Comprehension_Systems,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2017},
  pages = {2021--2031},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1215},
  url = {http://aclweb.org/anthology/D17-1215}
}





@inproceedings{McCoyPavlickEtAl_2019_Right_for_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  url = {https://www.aclweb.org/anthology/P19-1334}
}




@inproceedings{DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1423/},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = jun,
  year = {2019},
  pages = {4171-4186}
}



@article{YangDaiEtAl_2019_XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.08237},
  primaryClass = {cs},
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
  journal = {arXiv:1906.08237 [cs]},
  url = {http://arxiv.org/abs/1906.08237},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  month = jun,
  year = {2019}
}



@inproceedings{PetersNeumannEtAl_2018_Deep_Contextualized_Word_Representations,
  title = {Deep {{Contextualized Word Representations}}},
  language = {en-us},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  doi = {10.18653/v1/N18-1202},
  url = {https://aclweb.org/anthology/papers/N/N18/N18-1202/},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  month = jun,
  year = {2018},
  pages = {2227-2237}
}


@inproceedings{ZhangHanEtAl_2019_ERNIE_Enhanced_Language_Representation_with_Informative_Entities,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.07129},
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  shorttitle = {{{ERNIE}}},
  abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
  booktitle = {{{ACL}} 2019},
  url = {http://arxiv.org/abs/1905.07129},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  month = may,
  year = {2019}
}


@article{RadfordWuEtAl_2019_Language_models_are_unsupervised_multitask_learners,
  title = {Language Models Are Unsupervised Multitask Learners},
  volume = {1},
  journal = {OpenAI Blog},
  url = {https://openai.com/blog/better-language-models/},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  pages = {8}
}


@inproceedings{StrubellGaneshEtAl_2019_Energy_and_Policy_Considerations_for_Deep_Learning_in_NLP,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02243},
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  booktitle = {{{ACL}} 2019},
  url = {http://arxiv.org/abs/1906.02243},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  month = jun,
  year = {2019}
}


@inproceedings{WuFanEtAl_2019_Pay_Less_Attention_with_Lightweight_and_Dynamic_Convolutions,
  title = {Pay {{Less Attention}} with {{Lightweight}} and {{Dynamic Convolutions}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  url = {https://openreview.net/forum?id=SkVhlh09tX},
  author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
  year = {2019}
}


@inproceedings{FrankleCarbin_2019_Lottery_Ticket_Hypothesis_Finding_Sparse_Trainable_Neural_Networks,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  url = {https://openreview.net/forum?id=rJl-b3RcF7},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019}
}


@article{Goldberg_2019_Assessing_BERTs_Syntactic_Abilities,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.05287},
  primaryClass = {cs},
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
  journal = {arXiv:1901.05287 [cs]},
  url = {http://arxiv.org/abs/1901.05287},
  author = {Goldberg, Yoav},
  month = jan,
  year = {2019}
}


@inproceedings{LapesaEvert_2017_Large-scale_evaluation_of_dependency-based_DSMs_Are_they_worth_the_effort,
  title = {Large-Scale Evaluation of Dependency-Based {{DSMs}}: {{Are}} They Worth the Effort?},
  shorttitle = {Large-Scale Evaluation of Dependency-Based {{DSMs}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{EACL}})},
  publisher = {{Association for Computational Linguistics}},
  url = {http://www.aclweb.org/anthology/E17-2063},
  author = {Lapesa, Gabriella and Evert, Stefan},
  year = {2017},
  pages = {394-400}
}


@inproceedings{LiLiuEtAl_2017_Investigating_Different_Syntactic_Context_Types_and_Context_Representations_for_Learning_Word_Embeddings,
  address = {{Copenhagen, Denmark, September 7\textendash{}11, 2017}},
  title = {Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  url = {http://aclweb.org/anthology/D17-1257},
  author = {Li, Bofang and Liu, Tao and Zhao, Zhe and Tang, Buzhou and Drozd, Aleksandr and Rogers, Anna and Du, Xiaoyong},
  year = {2017},
  pages = {2411--2421}
}


@article{VoitaTalbotEtAl_2019_Analyzing_Multi-Head_Self-Attention_Specialized_Heads_Do_Heavy_Lifting_Rest_Can_Be_Pruned,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09418},
  primaryClass = {cs},
  title = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}: {{Specialized Heads Do}} the {{Heavy Lifting}}, the {{Rest Can Be Pruned}}},
  shorttitle = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}},
  abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
  journal = {arXiv:1905.09418 [cs]},
  url = {http://arxiv.org/abs/1905.09418},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  month = may,
  year = {2019}
}


@article{ClarkKhandelwalEtAl_2019_What_Does_BERT_Look_At_Analysis_of_BERTs_Attention,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.04341},
  primaryClass = {cs},
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  journal = {arXiv:1906.04341 [cs]},
  url = {http://arxiv.org/abs/1906.04341},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  month = jun,
  year = {2019}
}


@article{LinTanEtAl_2019_Open_Sesame_Getting_Inside_BERTs_Linguistic_Knowledge,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01698},
  primaryClass = {cs},
  title = {Open {{Sesame}}: {{Getting Inside BERT}}'s {{Linguistic Knowledge}}},
  shorttitle = {Open {{Sesame}}},
  abstract = {How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT's representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.},
  journal = {arXiv:1906.01698 [cs]},
  url = {http://arxiv.org/abs/1906.01698},
  author = {Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  month = jun,
  year = {2019}
}

@inproceedings{JawaharSagotEtAl_What_does_BERT_learn_about_structure_of_language,
  title = {What Does {{BERT}} Learn about the Structure of Language?},
  abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
  language = {en},
  booktitle = {{{ACL}} 2019},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  pages = {8}
}

@article{CoenenReifEtAl_2019_Visualizing_and_Measuring_Geometry_of_BERT,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02715},
  primaryClass = {cs, stat},
  title = {Visualizing and {{Measuring}} the {{Geometry}} of {{BERT}}},
  abstract = {Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
  journal = {arXiv:1906.02715 [cs, stat]},
  url = {http://arxiv.org/abs/1906.02715},
  author = {Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  month = jun,
  year = {2019}
}


@inproceedings{MikolovChenEtAl_2013_Efficient_estimation_of_word_representations_in_vector_space,
  title = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {Proceedings of {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  url = {https://arxiv.org/pdf/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
}


@inproceedings{MikolovYihEtAl_2013_Linguistic_Regularities_in_Continuous_Space_Word_Representations,
  address = {{Atlanta, Georgia, 9\textendash{}14 June 2013}},
  title = {Linguistic {{Regularities}} in {{Continuous Space Word Representations}}.},
  booktitle = {Proceedings of {{NAACL}}-{{HLT}} 2013},
  url = {https://www.aclweb.org/anthology/N13-1090},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  keywords = {_Sasha},
  pages = {746--751}
}


@inproceedings{KoperScheibleEtAl_2015_Multilingual_reliability_and_semantic_structure_of_continuous_word_spaces,
  title = {Multilingual Reliability and "Semantic" Structure of Continuous Word Spaces},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Computational Semantics}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://www.aclweb.org/anthology/W15-01\#page=56},
  author = {K{\"o}per, Maximilian and Scheible, Christian and {im Walde}, Sabine Schulte},
  year = {2015},
  pages = {40-45}
}


@inproceedings{VylomovaRimmelEtAl_2016_Take_and_took_gaggle_and_goose_book_and_read_evaluating_utility_of_vector_differences_for_lexical_relation_learning,
  address = {{Berlin, Germany}},
  title = {Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning},
  shorttitle = {Take and {{Took}}, {{Gaggle}} and {{Goose}}, {{Book}} and {{Read}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P16-1158},
  url = {http://www.aclweb.org/anthology/P16-1158},
  author = {Vylomova, Ekaterina and Rimmel, Laura and Cohn, Trevor and Baldwin, Timothy},
  year = {2016},
  keywords = {_Sasha},
  pages = {1671--1682}
}



@inproceedings{KarpinskaLiEtAl_2018_Subcharacter_Information_in_Japanese_Embeddings_When_Is_It_Worth_It,
  address = {{Melbourne, Australia}},
  title = {Subcharacter {{Information}} in {{Japanese Embeddings}}: {{When Is It Worth It}}?},
  booktitle = {Proceedings of the {{Workshop}} on the {{Relevance}} of {{Linguistic Structure}} in {{Neural Architectures}} for {{NLP}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/W18-2905},
  author = {Karpinska, Marzena and Li, Bofang and Rogers, Anna and Drozd, Aleksandr},
  year = {2018},
  pages = {28-37}
}


@inproceedings{GladkovaDrozdEtAl_2016_Analogybased_detection_of_morphological_and_semantic_relations_with_word_embeddings_what_works_and_what_doesnt,
  address = {{San Diego, California, June 12-17, 2016}},
  title = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings: What Works and What Doesn't.},
  booktitle = {Proceedings of the {{NAACL}}-{{HLT SRW}}},
  publisher = {{ACL}},
  doi = {10.18653/v1/N16-2002},
  url = {https://www.aclweb.org/anthology/N/N16/N16-2002.pdf},
  author = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
  year = {2016},
  keywords = {peer-reviewed},
  pages = {47-54}
}


@inproceedings{Linzen_2016_Issues_in_evaluating_semantic_spaces_using_word_analogies,
  title = {Issues in Evaluating Semantic Spaces Using Word Analogies.},
  booktitle = {Proceedings of the {{First Workshop}} on {{Evaluating Vector Space Representations}} for {{NLP}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {http://dx.doi.org/10.18653/v1/W16-2503},
  url = {http://anthology.aclweb.org/W16-2503},
  author = {Linzen, Tal},
  year = {2016}
}


@inproceedings{GittensAchlioptasEtAl_2017_SkipGram_Zipf_Uniform_Vector_Additivity,
  title = {Skip-{{Gram}} - {{Zipf}} + {{Uniform}} = {{Vector Additivity}}},
  language = {en-us},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  doi = {10.18653/v1/P17-1007},
  url = {https://www.aclweb.org/anthology/papers/P/P17/P17-1007/},
  author = {Gittens, Alex and Achlioptas, Dimitris and Mahoney, Michael W.},
  month = jul,
  year = {2017},
  pages = {69-76}
}

@article{EthayarajhDuvenaudEtAl_2019_Towards_Understanding_Linear_Word_Analogies,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04882},
  title = {Towards {{Understanding Linear Word Analogies}}},
  abstract = {A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.},
  journal = {To appear in ACL 2019},
  url = {http://arxiv.org/abs/1810.04882},
  author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
  month = oct,
  year = {2019}
}


@inproceedings{Schluter_2018_Word_Analogy_Testing_Caveat,
  title = {The {{Word Analogy Testing Caveat}}},
  language = {en-us},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  doi = {10.18653/v1/N18-2039},
  url = {https://www.aclweb.org/anthology/papers/N/N18/N18-2039/},
  author = {Schluter, Natalie},
  month = jun,
  year = {2018},
  pages = {242-246}
}



@inproceedings{WashioKato_2018_Neural_Latent_Relational_Analysis_to_Capture_Lexical_Semantic_Relations_in_a_Vector_Space,
  address = {{Brussels, Belgium}},
  title = {Neural {{Latent Relational Analysis}} to {{Capture Lexical Semantic Relations}} in a {{Vector Space}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/D18-1058},
  author = {Washio, Koki and Kato, Tsuneaki},
  year = {2018},
  pages = {594-600}
}

@article{JoshiChoiEtAl_2018_pair2vec_Compositional_Word-Pair_Embeddings_for_Cross-Sentence_Inference,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.08854},
  primaryClass = {cs},
  title = {Pair2vec: {{Compositional Word}}-{{Pair Embeddings}} for {{Cross}}-{{Sentence Inference}}},
  shorttitle = {Pair2vec},
  abstract = {Reasoning about implied relationships (e.g. paraphrastic, common sense, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function of each word's representation, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the the two words co-occur. We add these representations to the cross-sentence attention layer of existing inference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or replacing existing word embeddings. Experiments show a gain of 2.72\% on the recently released SQuAD 2.0 and 1.3\% on MultiNLI. Our representations also aid in better generalization with gains of around 6-7\% on adversarial SQuAD datasets, and 8.8\% on the adversarial entailment test set by Glockner et al.},
  journal = {arXiv:1810.08854 [cs]},
  url = {http://arxiv.org/abs/1810.08854},
  author = {Joshi, Mandar and Choi, Eunsol and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
  month = oct,
  year = {2018}
}

@article{Camacho-ColladosEspinosa-AnkeEtAl_2019_Relational_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01373},
  title = {Relational {{Word Embeddings}}},
  abstract = {While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.},
  journal = {ACL 2019},
  url = {http://arxiv.org/abs/1906.01373},
  author = {{Camacho-Collados}, Jose and {Espinosa-Anke}, Luis and Schockaert, Steven},
  month = jun,
  year = {2019}
}


@inproceedings{HakamiHayashiEtAl_2018_Why_does_PairDiff_work,
  title = {Why Does {{PairDiff}} Work? - {{A Mathematical Analysis}} of {{Bilinear Relational Compositional Operators}} for {{Analogy Detection}}},
  shorttitle = {Why Does {{PairDiff}} Work?},
  language = {en-us},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  url = {https://www.aclweb.org/anthology/papers/C/C18/C18-1211/},
  author = {Hakami, Huda and Hayashi, Kohei and Bollegala, Danushka},
  month = aug,
  year = {2018},
  pages = {2493-2504}
}


@inproceedings{DrozdGladkovaEtAl_2016_Word_embeddings_analogies_and_machine_learning_beyond_king_man_woman_queen,
  address = {{Osaka, Japan, December 11-17}},
  title = {Word Embeddings, Analogies, and Machine Learning: Beyond King - Man + Woman = Queen},
  shorttitle = {Word {{Embeddings}}, {{Analogies}}, and {{Machine Learning}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  url = {https://www.aclweb.org/anthology/C/C16/C16-1332.pdf},
  author = {Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
  year = {2016},
  keywords = {peer-reviewed},
  pages = {3519--3530}
}


@article{NissimvanNoordEtAl_2019_Fair_is_Better_than_SensationalMan_is_to_Doctor_as_Woman_is_to_Doctor,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09866},
  primaryClass = {cs},
  title = {Fair Is {{Better}} than {{Sensational}}:{{Man}} Is to {{Doctor}} as {{Woman}} Is to {{Doctor}}},
  shorttitle = {Fair Is {{Better}} than {{Sensational}}},
  abstract = {Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also exposed how strongly human biases are encoded in vector spaces built on natural language. While finding that queen is the answer to man is to king as woman is to X leaves us in awe, papers have also reported finding analogies deeply infused with human biases, like man is to computer programmer as woman is to homemaker, which instead leave us with worry and rage. In this work we show that,often unknowingly, embedding spaces have not been treated fairly. Through a series of simple experiments, we highlight practical and theoretical problems in previous works, and demonstrate that some of the most widely used biased analogies are in fact not supported by the data. We claim that rather than striving to find sensational biases, we should aim at observing the data "as is", which is biased enough. This should serve as a fair starting point to properly address the evident, serious, and compelling problem of human bias in word embeddings.},
  journal = {arXiv:1905.09866 [cs]},
  url = {http://arxiv.org/abs/1905.09866},
  author = {Nissim, Malvina and {van Noord}, Rik and {van der Goot}, Rob},
  month = may,
  year = {2019}
}


@inproceedings{BolukbasiChangEtAl_2016_Man_is_to_Computer_Programmer_As_Woman_is_to_Homemaker_Debiasing_Word_Embeddings,
  address = {{USA}},
  series = {{{NIPS}}'16},
  title = {Man Is to {{Computer Programmer As Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  isbn = {978-1-5108-3881-9},
  shorttitle = {Man Is to {{Computer Programmer As Woman}} Is to {{Homemaker}}?},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Curran Associates Inc.}},
  url = {http://dl.acm.org/citation.cfm?id=3157382.3157584},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  pages = {4356--4364}
}


@inproceedings{RogersDrozdEtAl_2017_Too_Many_Problems_of_Analogical_Reasoning_with_Word_Vectors,
  title = {The ({{Too Many}}) {{Problems}} of {{Analogical Reasoning}} with {{Word Vectors}}},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (* {{SEM}} 2017)},
  url = {http://www.aclweb.org/anthology/S17-1017},
  author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
  year = {2017},
  keywords = {peer-reviewed},
  pages = {135--148}
}


@inproceedings{ThomasonGordonEtAl_2019_Shifting_Baseline_Single_Modality_Performance_on_Visual_Navigation_QA,
  title = {Shifting the {{Baseline}}: {{Single Modality Performance}} on {{Visual Navigation}} \& {{QA}}},
  shorttitle = {Shifting the {{Baseline}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1197/},
  author = {Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan},
  month = jun,
  year = {2019},
  pages = {1977-1983}
}


@inproceedings{Yatskar_2019_Qualitative_Comparison_of_CoQA_SQuAD_20_and_QuAC,
  title = {A {{Qualitative Comparison}} of {{CoQA}}, {{SQuAD}} 2.0 and {{QuAC}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1241/},
  author = {Yatskar, Mark},
  month = jun,
  year = {2019},
  pages = {2318-2323}
}


@inproceedings{JainWallace_2019_Attention_is_not_Explanation,
  title = {Attention Is Not {{Explanation}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1357/},
  author = {Jain, Sarthak and Wallace, Byron C.},
  month = jun,
  year = {2019},
  pages = {3543-3556}
}


@inproceedings{VineGevaEtAl_2018_Unsupervised_Mining_of_Analogical_Frames_by_Constraint_Satisfaction,
  title = {Unsupervised {{Mining}} of {{Analogical Frames}} by {{Constraint Satisfaction}}},
  language = {en-us},
  booktitle = {Proceedings of the {{Australasian Language Technology Association Workshop}} 2018},
  url = {https://www.aclweb.org/anthology/papers/U/U18/U18-1004/},
  author = {Vine, Lance De and Geva, Shlomo and Bruza, Peter},
  month = dec,
  year = {2018},
  pages = {34-43}
}

@inproceedings{BouraouiJameelEtAl_2018_Relation_Induction_in_Word_Embeddings_Revisited,
  title = {Relation {{Induction}} in {{Word Embeddings Revisited}}},
  language = {en-us},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  url = {https://www.aclweb.org/anthology/papers/C/C18/C18-1138/},
  author = {Bouraoui, Zied and Jameel, Shoaib and Schockaert, Steven},
  month = aug,
  year = {2018},
  pages = {1627-1637}
}

@article{DufterSchutze_2019_Analytical_Methods_for_Interpretable_Ultradense_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.08654},
  primaryClass = {cs},
  title = {Analytical {{Methods}} for {{Interpretable Ultradense Word Embeddings}}},
  abstract = {Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. While DensRay is very closely related to the Densifier, it can be computed in closed form, is hyperparameter-free and thus more robust than the Densifier. We evaluate the methods on lexicon induction and set-based word analogy and conclude that analytical methods such as DensRay and SVMs are preferable. For word analogy we propose a new method to solve the task which outperforms the previous state of the art by large margins.},
  journal = {arXiv:1904.08654 [cs]},
  url = {http://arxiv.org/abs/1904.08654},
  author = {Dufter, Philipp and Sch{\"u}tze, Hinrich},
  month = apr,
  year = {2019}
}


@inproceedings{HakamiBollegala_2019_Learning_Relation_Representations_from_Word_Representations,
  title = {Learning {{Relation Representations}} from {{Word Representations}}},
  abstract = {Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical...},
  booktitle = {{{AKBC}} 2019},
  url = {https://openreview.net/forum?id=r1e3WW5aTX\&noteId=BklR5HOySN},
  author = {Hakami, Huda and Bollegala, Danushka},
  year = {2019}
}



@article{AllenHospedales_2019_Analogies_Explained_Towards_Understanding_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09813},
  primaryClass = {cs, stat},
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  shorttitle = {Analogies {{Explained}}},
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of "\$w\_x\$ is to \$w\_y\$". From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
  journal = {arXiv:1901.09813 [cs, stat]},
  url = {http://arxiv.org/abs/1901.09813},
  author = {Allen, Carl and Hospedales, Timothy},
  month = jan,
  year = {2019}
}



@misc{Greene_2019_How_Twitter_shapes_global_public_conversation_Jack_Dorsey_speaks_at_TED2019,
  title = {How {{Twitter}} Shapes Global Public Conversation: {{Jack Dorsey}} Speaks at {{TED2019}}},
  shorttitle = {How {{Twitter}} Shapes Global Public Conversation},
  abstract = {At TED2019, head of TED Chris Anderson and TED current affairs curator Whitney Pennington Rodgers join Twitter and Square CEO Jack Dorsey to discuss the health of the global conversation and how Twitter could change what it incentivizes users to do -- moving away from outrage and mob behavior and towards productive, healthy conversation.},
  language = {en},
  journal = {TED Blog},
  url = {https://blog.ted.com/how-twitter-shapes-global-public-conversation-jack-dorsey-speaks-at-ted2019/},
  author = {Greene, Brown},
  month = apr,
  year = {2019}
}

@misc{Farokhmanesh_2018_What_remains_of_Chatroulette_its_gone_to_dongs,
  title = {What Remains of {{Chatroulette}}: It's Gone to the Do(n)Gs},
  shorttitle = {What Remains of {{Chatroulette}}},
  abstract = {A small community persists on the once popular site},
  journal = {The Verge},
  url = {https://www.theverge.com/2018/2/14/16381942/chatroulette-webcam-chat-male-community},
  author = {Farokhmanesh, Megan},
  month = feb,
  year = {2018}
}

@article{BozdagvandenHoven_2015_Breaking_filter_bubble_democracy_and_design,
  title = {Breaking the Filter Bubble: Democracy and Design},
  volume = {17},
  issn = {1572-8439},
  shorttitle = {Breaking the Filter Bubble},
  abstract = {It has been argued that the Internet and social media increase the number of available viewpoints, perspectives, ideas and opinions available, leading to a very diverse pool of information. However, critics have argued that algorithms used by search engines, social networking platforms and other large online intermediaries actually decrease information diversity by forming so-called ``filter bubbles''. This may form a serious threat to our democracies. In response to this threat others have developed algorithms and digital tools to combat filter bubbles. This paper first provides examples of different software designs that try to break filter bubbles. Secondly, we show how norms required by two democracy models dominate the tools that are developed to fight the filter bubbles, while norms of other models are completely missing in the tools. The paper in conclusion argues that democracy itself is a contested concept and points to a variety of norms. Designers of diversity enhancing tools must thus be exposed to diverse conceptions of democracy.},
  language = {en},
  number = {4},
  journal = {Ethics and Information Technology},
  doi = {10.1007/s10676-015-9380-y},
  url = {https://doi.org/10.1007/s10676-015-9380-y},
  author = {Bozdag, Engin and {van den Hoven}, Jeroen},
  month = dec,
  year = {2015},
  pages = {249-265}
}

@book{Pariser_2011_filter_bubble_what_Internet_is_hiding_from_you,
  address = {{New York, NY}},
  title = {The Filter Bubble: What the {{Internet}} Is Hiding from You},
  isbn = {978-1-59420-300-8 978-0-14-312123-7},
  shorttitle = {The Filter Bubble},
  abstract = {In December 2009, Google began customizing its search results for each user. Instead of giving you the most broadly popular result, Google now tries to predict what you are most likely to click on. According to MoveOn.org board president Eli Pariser, Google's change in policy is symptomatic of the most significant shift to take place on the Web in recent years-the rise of personalization. In this groundbreaking investigation of the new hidden Web, Pariser uncovers how this growing trend threatens to control how we consume and share information as a society-and reveals what we can do about it. Personalized filters are sweeping the Web, creating individual universes of information for each of us. Facebook prioritizes the links it believes will appeal to you so that if you are a liberal, you can expect to see only progressive links. Behind the scenes a burgeoning industry of data companies is tracking your personal information to sell to advertisers, from your political leanings to the color you painted your living room to the hiking boots you just browsed on Zappos. In a personalized world, we will increasingly be typed and fed only news that is pleasant, familiar, and confirms our beliefs-and because these filters are invisible, we won't know what is being hidden from us. Our past interests will determine what we are exposed to in the future, leaving less room for the unexpected encounters that spark creativity, innovation, and the democratic exchange of ideas. While we all worry that the Internet is eroding privacy or shrinking our attention spans, Pariser uncovers a more pernicious and far- reaching trend on the Internet and shows how we can- and must-change course. With vivid detail and remarkable scope, The Filter Bubble reveals how personalization undermines the Internet's original purpose as an open platform for the spread of ideas and could leave us all in an isolated, echoing world.},
  language = {English},
  publisher = {{Penguin Press}},
  author = {Pariser, Eli},
  year = {2011}
}

@misc{Keegan_Blue_Feed_Red_Feed,
  title = {Blue {{Feed}}, {{Red Feed}}},
  abstract = {See Liberal Facebook and Conservative Facebook, Side by Side},
  language = {en},
  journal = {WSJ},
  url = {http://graphics.wsj.com/blue-feed-red-feed/},
  author = {Keegan, Jon}
}

@misc{MacLellan_2016_scientifically_proven_step-by-step_guide_to_having_breakthrough_conversation_across_party_lines,
  title = {The Scientifically Proven, Step-by-Step Guide to Having a Breakthrough Conversation across Party Lines},
  abstract = {A conflict resolution strategy honed in the US civil rights era is back in vogue.},
  language = {en},
  journal = {Quartz},
  url = {https://qz.com/838321/nonviolent-communication-the-scientifically-proven-step-by-step-guide-to-having-a-breakthrough-conversation-across-party-lines/},
  author = {MacLellan, Lila},
  month = nov,
  year = {2016}
}

@misc{Adee_2016_How_can_Facebook_and_its_users_burst_filter_bubble,
  title = {How Can {{Facebook}} and Its Users Burst the 'Filter Bubble'?},
  abstract = {Social media filter bubbles have come under scrutiny following the US election. Design tweaks and new habits could help pop them and expand our views

\&nbsp;},
  language = {en-US},
  journal = {New Scientist},
  url = {https://www.newscientist.com/article/2113246-how-can-facebook-and-its-users-burst-the-filter-bubble/},
  author = {Adee, Sally},
  month = nov,
  year = {2016}
}

@misc{Pariser_2011_Beware_online_filter_bubbles,
  title = {Beware Online "Filter Bubbles"},
  abstract = {As web companies strive to tailor their services (including news and search results) to our personal tastes, there's a dangerous unintended consequence: We get trapped in a "filter bubble" and don't get exposed to information that could challenge or broaden our worldview. Eli Pariser argues powerfully that this will ultimately prove to be bad for us and bad for democracy.},
  language = {en},
  url = {https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles},
  author = {Pariser, Eli},
  year = {2011}
}

@misc{Pachal_2016_How_to_improve_your_Facebook_feed_so_we_see_next_Trump_coming,
  title = {How to Improve Your {{Facebook}} Feed, so We See the next {{Trump}} Coming},
  abstract = {Time to pop the filter bubble.},
  language = {en},
  journal = {Mashable},
  url = {https://mashable.com/2016/11/17/avoid-facebook-filter-bubble/},
  author = {Pachal, Pete},
  month = nov,
  year = {2016}
}

@misc{Chen_2018_Cambridge_Analyticas_Facebook_data_abuse_shouldnt_get_credit_for_Trump,
  title = {Cambridge {{Analytica}}'s {{Facebook}} Data Abuse Shouldn't Get Credit for {{Trump}}},
  abstract = {"I think Cambridge Analytica is a better marketing company than a targeting company."},
  journal = {The Verge},
  url = {https://www.theverge.com/2018/3/20/17138854/cambridge-analytica-facebook-data-trump-campaign-psychographic-microtargeting},
  author = {Chen, Angela},
  month = mar,
  year = {2018}
}

@misc{Read_Across_Aisle,
  title = {Read {{Across The Aisle}}},
  abstract = {Ever wonder what your social media news feed isn't showing you? Escape your ``filter bubble'' with Read Across The Aisle, a new way to combat political polarization and stay informed!},
  language = {en},
  journal = {Read Across The Aisle},
  url = {www.readacrosstheaisle.com}
}

@misc{2019_AllSides_Balanced_news_via_media_bias_ratings_for_unbiased_news_perspective,
  title = {{{AllSides}} | {{Balanced}} News via Media Bias Ratings for an Unbiased News Perspective},
  abstract = {See issues and political news with news bias revealed. Non-partisan, crowd-sourced technology shows all sides so you can decide.},
  language = {en},
  journal = {AllSides},
  url = {https://www.allsides.com/unbiased-balanced-news},
  month = may,
  year = {2019}
}

@misc{Lumb_2015_Why_Scientists_Are_Upset_About_Facebook_Filter_Bubble_Study,
  title = {Why {{Scientists Are Upset About The Facebook Filter Bubble Study}}},
  abstract = {Facebook says users, not its algorithm, are responsible for the so-called filter bubble. But Facebook's own research says otherwise.},
  language = {en-US},
  journal = {Fast Company},
  url = {https://www.fastcompany.com/3046111/why-scientists-are-upset-over-the-facebook-filter-bubble-study},
  author = {Lumb, David},
  month = may,
  year = {2015}
}

@article{BakshyMessingEtAl_2015_Exposure_to_ideologically_diverse_news_and_opinion_on_Facebook,
  title = {Exposure to Ideologically Diverse News and Opinion on {{Facebook}}},
  volume = {348},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  abstract = {Not getting all sides of the news?
People are increasingly turning away from mass media to social media as a way of learning news and civic information. Bakshy et al. examined the news that millions of Facebook users' peers shared, what information these users were presented with, and what they ultimately consumed (see the Perspective by Lazer). Friends shared substantially less cross-cutting news from sources aligned with an opposing ideology. People encountered roughly 15\% less cross-cutting content in news feeds due to algorithmic ranking and clicked through to 70\% less of this cross-cutting content. Within the domain of political news encountered in social media, selective exposure appears to drive attention.
Science, this issue p. 1130; see also p. 1090
Exposure to news, opinion, and civic information increasingly occurs through social media. How do these online networks influence exposure to perspectives that cut across ideological lines? Using deidentified data, we examined how 10.1 million U.S. Facebook users interact with socially shared news. We directly measured ideological homophily in friend networks and examined the extent to which heterogeneous friends could potentially expose individuals to cross-cutting content. We then quantified the extent to which individuals encounter comparatively more or less diverse content while interacting via Facebook's algorithmically ranked News Feed and further studied users' choices to click through to ideologically discordant content. Compared with algorithmic ranking, individuals' choices played a stronger role in limiting exposure to cross-cutting content.
Despite the diversity of information available, people still pay attention to a limited range of opinions. [Also see Perspective by Lazer]
Despite the diversity of information available, people still pay attention to a limited range of opinions. [Also see Perspective by Lazer]},
  language = {en},
  number = {6239},
  journal = {Science},
  doi = {10.1126/science.aaa1160},
  url = {https://science.sciencemag.org/content/348/6239/1130},
  author = {Bakshy, Eytan and Messing, Solomon and Adamic, Lada A.},
  month = jun,
  year = {2015},
  pages = {1130-1132},
  pmid = {25953820}
}

@misc{Tam_2018_Pop_Goes_Filter_Bubble,
  title = {Pop {{Goes}} the {{Filter Bubble}}?},
  abstract = {Pop goes the filter bubble?},
  language = {en},
  journal = {Slate Magazine},
  url = {https://slate.com/technology/2018/04/personalization-algorithms-could-actually-help-pop-filter-bubbles.html},
  author = {Tam, Stephanie},
  month = apr,
  year = {2018}
}

@article{DeCarbonnel_2019_Exclusive_Echo_chambers_-_Fake_news_fact-checks_hobbled_by_low,
  title = {Exclusive: {{Echo}} Chambers - {{Fake}} News Fact-Checks Hobbled by Low...},
  shorttitle = {Exclusive},
  abstract = {The European Union has called on Facebook and other platforms to invest more in ...},
  language = {en},
  journal = {Reuters},
  url = {https://www.reuters.com/article/us-eu-disinformation-exclusive-idUSKCN1U60PT},
  author = {De Carbonnel, Alissa},
  month = jul,
  year = {2019}
}


@inproceedings{GillaniYuanEtAl_2018_Me_My_Echo_Chamber_and_I_Introspection_on_Social_Media_Polarization,
  address = {{Republic and Canton of Geneva, Switzerland}},
  series = {{{WWW}} '18},
  title = {Me, {{My Echo Chamber}}, and {{I}}: {{Introspection}} on {{Social Media Polarization}}},
  isbn = {978-1-4503-5639-8},
  shorttitle = {Me, {{My Echo Chamber}}, and {{I}}},
  abstract = {Homophily - our tendency to surround ourselves with others who share our perspectives and opinions about the world - is both a part of human nature and an organizing principle underpinning many of our digital social networks. However, when it comes to politics or culture, homophily can amplify tribal mindsets and produce "echo chambers" that degrade the quality, safety, and diversity of discourse online. While several studies have empirically proven this point, few have explored how making users aware of the extent and nature of their political echo chambers influences their subsequent beliefs and actions. In this paper, we introduce Social Mirror, a social network visualization tool that enables a sample of Twitter users to explore the politically-active parts of their social network. We use Social Mirror to recruit Twitter users with a prior history of political discourse to a randomized experiment where we evaluate the effects of different treatments on participants' i) beliefs about their network connections, ii) the political diversity of who they choose to follow, and iii) the political alignment of the URLs they choose to share. While we see no effects on average political alignment of shared URLs, we find that recommending accounts of the opposite political ideology to follow reduces participants\guillemotright{} beliefs in the political homogeneity of their network connections but still enhances their connection diversity one week after treatment. Conversely, participants who enhance their belief in the political homogeneity of their Twitter connections have less diverse network connections 2-3 weeks after treatment. We explore the implications of these disconnects between beliefs and actions on future efforts to promote healthier exchanges in our digital public spheres.},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186130},
  url = {https://doi.org/10.1145/3178876.3186130},
  author = {Gillani, Nabeel and Yuan, Ann and Saveski, Martin and Vosoughi, Soroush and Roy, Deb},
  year = {2018},
  pages = {823--831}
}


@article{DodgeIlharcoEtAl_2020_Fine-Tuning_Pretrained_Language_Models_Weight_Initializations_Data_Orders_and_Early_Stopping,
  title = {Fine-{{Tuning Pretrained Language Models}}: {{Weight Initializations}}, {{Data Orders}}, and {{Early Stopping}}},
  shorttitle = {Fine-{{Tuning Pretrained Language Models}}},
  author = {Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  year = {2020},
  url = {http://arxiv.org/abs/2002.06305},
  archivePrefix = {arXiv},
  journal = {arXiv:2002.06305 [cs]},
  primaryClass = {cs}
}


@article{LiuOttEtAl_2019_RoBERTa_Robustly_Optimized_BERT_Pretraining_Approach,
  ids = {LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approach,LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approacha},
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  url = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.11692 [cs]},
  primaryClass = {cs}
}


@inproceedings{DodgeGururanganEtAl_2019_Show_Your_Work_Improved_Reporting_of_Experimental_Results,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  pages = {2185--2194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1224},
  url = {https://www.aclweb.org/anthology/D19-1224}
}


@inproceedings{DodgeGururanganEtAl_2019_Show_Your_Work_Improved_Reporting_of_Experimental_Results,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  pages = {2185--2194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1224},
  url = {https://www.aclweb.org/anthology/D19-1224}
}


@article{Anderson_2009_Conference_reviewing_considered_harmful,
  title = {Conference Reviewing Considered Harmful},
  author = {Anderson, Thomas},
  year = {2009},
  volume = {43},
  pages = {108},
  issn = {01635980},
  doi = {10.1145/1531793.1531815},
  url = {http://portal.acm.org/citation.cfm?doid=1531793.1531815},
  journal = {ACM SIGOPS Operating Systems Review},
  number = {2}
}





@article{SchwartzDodgeEtAl_2019_Green_AI,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  url = {http://arxiv.org/abs/1907.10597},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.10597 [cs, stat]},
  primaryClass = {cs, stat}
}





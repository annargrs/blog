@article{RogersGardnerEtAl_2023_QA_Dataset_Explosion_Taxonomy_of_NLP_Resources_for_Question_Answering_and_Reading_Comprehension,
  title = {{{QA Dataset Explosion}}: {{A Taxonomy}} of {{NLP Resources}} for {{Question Answering}} and {{Reading Comprehension}}},
  shorttitle = {{{QA Dataset Explosion}}},
  author = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
  year = {2023},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {10},
  pages = {197:1--197:45},
  issn = {0360-0300},
  doi = {10.1145/3560260},
  url={https://arxiv.org/abs/2107.12708},
  urldate = {2023-05-22}
}

@article{bommasani2021opportunities,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.07258 [cs]},
  eprint = {2108.07258},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2021-08-18},
  archiveprefix = {arXiv}
}


@inproceedings{BrownMannEtAl_2020_Language_Models_are_Few-Shot_Learners,
  ids = {BrownMannEtAl_2020_Language_Models_are_Few-Shot_Learnersa},
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33 ({{NeurIPS}} 2020)},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jun,
  eprint = {2005.14165},
  url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2020-06-04},
  archiveprefix = {arXiv}
}


@inproceedings{schaeffer2023emergent,
  title = {Are Emergent Abilities of Large Language Models a Mirage?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  year = {2023},
  volume = {36},
  pages = {55565--55581},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/adc98a266f45005c403b8311ca7e8bd7-Paper-Conference.pdf}
}

@inproceedings{deshpande-etal-2023-honey,
  title = "Honey, {I} Shrunk the Language: Language Model Behavior at Reduced Scale.",
  author = "Deshpande, Vijeta  and
  Pechi, Dan  and
  Thatte, Shree  and
  Lialin, Vladislav  and
  Rumshisky, Anna",
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  pages = "5298--5314",
  url={https://aclanthology.org/2023.findings-acl.326},
  abstract = "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as {\textasciitilde}1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \times 10^{15}$ FLOPs. We also find that adding layers does not always benefit downstream performance.Our filtered pre-training data, reduced English vocabulary, and code are available at https://github.com/text-machine-lab/mini{\_}bert$github.com/text-machine-lab/mini\_bert$",
}

@article{WeiTayEtAl_2022_Emergent_Abilities_of_Large_Language_Models,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  journal = {Transactions on Machine Learning Research},
  url={https://openreview.net/pdf?id=yzkSU5zdwD},
  issn = {2835-8856}
}

@misc{Wei_2023_Common_arguments_regarding_emergent_abilities,
  title = {Common Arguments Regarding Emergent Abilities},
  author = {Wei, Jason},
  year = {2023},
  month = may,
  journal = {Jason Wei's Blog},
  url = {https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities},
  urldate = {2024-05-20},
  langid = {american}
}

@misc{McCoyYaoEtAl_2023_Embers_of_Autoregression_Understanding_Large_Language_Models_Through_Problem_They_are_Trained_to_Solve,
  title = {Embers of {{Autoregression}}: {{Understanding Large Language Models Through}} the {{Problem They}} Are {{Trained}} to {{Solve}}},
  shorttitle = {Embers of {{Autoregression}}},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.13638},
  eprint = {2309.13638},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.13638},
  url={https://arxiv.org/abs/2309.13638},
  urldate = {2024-02-06},
  archiveprefix = {arXiv}
}

@article{ChanSantoroEtAl_2022_Data_Distributional_Properties_Drive_Emergent_In-Context_Learning_in_Transformers,
  title = {Data {{Distributional Properties Drive Emergent In-Context Learning}} in {{Transformers}}},
  author = {Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18878--18891},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/77c6ccacfd9962e2307fc64680fc5ace-Abstract-Conference.html},
  urldate = {2023-11-22},
  langid = {english},
  keywords = {ObsCite}
}

@inproceedings{MichaelHoltzmanEtAl_2023_What_Do_NLP_Researchers_Believe_Results_of_NLP_Community_Metasurvey,
  title = {What {{Do}} {{NLP}} {{Researchers Believe}}? {{Results}} of the {{NLP}} {{Community Metasurvey}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Michael, Julian and Holtzman, Ari and Parrish, Alicia and Mueller, Aaron and Wang, Alex and Chen, Angelica and Madaan, Divyam and Nangia, Nikita and Pang, Richard Yuanzhe and Phang, Jason and Bowman, Samuel R.},
  year = {2023},
  month = jul,
  pages = {16334--16368},
  publisher = {Association for Computational Linguistics},
  url={https://aclanthology.org/2023.acl-long.903/},
  address = {Toronto, Canada}
}


@inproceedings{SapLeBrasEtAl_2022_Neural_Theory-of-Mind_On_Limits_of_Social_Intelligence_in_Large_LMs,
  title = {Neural {{Theory-of-Mind}}? {{On}} the {{Limits}} of {{Social Intelligence}} in {{Large LMs}}},
  shorttitle = {Neural {{Theory-of-Mind}}?},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Sap, Maarten and Le Bras, Ronan and Fried, Daniel and Choi, Yejin},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {3762--3780},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.248},
  url={https://aclanthology.org/2022.emnlp-main.248},  
  urldate = {2024-07-15}
}


@misc{ShapiraLevyEtAl_2023_Clever_Hans_or_Neural_Theory_of_Mind_Stress_Testing_Social_Reasoning_in_Large_Language_Models,
  title = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}? {{Stress Testing Social Reasoning}} in {{Large Language Models}}},
  shorttitle = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}?},
  author = {Shapira, Natalie and Levy, Mosh and Alavi, Seyed Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
  year = {2023},
  month = may,
  number = {arXiv:2305.14763},
  eprint = {2305.14763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14763},
  url={https://arxiv.org/abs/2305.14763},  
  urldate = {2023-11-22},
  archiveprefix = {arXiv}
}


@misc{LiuNingEtAl_2023_Evaluating_Logical_Reasoning_Ability_of_ChatGPT_and_GPT-4,
  title = {Evaluating the {{Logical Reasoning Ability}} of {{ChatGPT}} and {{GPT-4}}},
  author = {Liu, Hanmeng and Ning, Ruoxi and Teng, Zhiyang and Liu, Jian and Zhou, Qiji and Zhang, Yue},
  year = {2023},
  month = may,
  number = {arXiv:2304.03439},
  eprint = {2304.03439},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.03439},
  url={https://arxiv.org/abs/2304.03439},    
  urldate = {2023-06-21},
  archiveprefix = {arXiv}
}


@misc{LuBigoulaevaEtAl_2023_Are_Emergent_Abilities_in_Large_Language_Models_just_In-Context_Learning,
  title = {Are {{Emergent Abilities}} in {{Large Language Models}} Just {{In-Context Learning}}?},
  author = {Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
  year = {2023},
  month = sep,
  number = {arXiv:2309.01809},
  eprint = {2309.01809},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.01809},
  urldate = {2023-11-22},
  url={https://arxiv.org/abs/2309.01809},     
  archiveprefix = {arXiv}
}


@misc{LiangBommasaniEtAl_2022_Holistic_Evaluation_of_Language_Models,
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R{\'e}, Christopher and {Acosta-Navas}, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09110},
  eprint = {2211.09110},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.09110},
  urldate = {2023-07-28},
  url={http://arxiv.org/abs/2211.09110},
  archiveprefix = {arXiv},
  keywords = {!}
}

@article{Veres_2022_Large_Language_Models_are_Not_Models_of_Natural_Language_They_are_Corpus_Models,
  title = {Large {{Language Models}} Are {{Not Models}} of {{Natural Language}}: {{They}} Are {{Corpus Models}}},
  shorttitle = {Large {{Language Models}} Are {{Not Models}} of {{Natural Language}}},
  author = {Veres, Csaba},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {61970--61979},
  issn = {2169-3536},
  url={https://ieeexplore.ieee.org/abstract/document/9794684},
  doi = {10.1109/ACCESS.2022.3182505},
  keywords = {c/position,dl/llm,fw/formal,g/public,q/soc/hype}
}

@inproceedings{ZhaoWallaceEtAl_2021_Calibrate_Before_Use_Improving_Few-shot_Performance_of_Language_Models,
  title = {Calibrate {{Before Use}}: {{Improving Few-shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  year = {2021},
  month = jul,
  pages = {12697--12706},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/zhao21c.html},
  urldate = {2022-06-24},
  langid = {english},
  keywords = {!}
}

@misc{BubeckChandrasekaranEtAl_2023_Sparks_of_Artificial_General_Intelligence_Early_experiments_with_GPT-4,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  url={http://arxiv.org/abs/2303.12712},  
  urldate = {2023-04-29},
  archiveprefix = {arXiv}
}

@inproceedings{LuBartoloEtAl_2022_Fantastically_Ordered_Prompts_and_Where_to_Find_Them_Overcoming_Few-Shot_Prompt_Order_Sensitivity,
  title = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}: {{Overcoming Few-Shot Prompt Order Sensitivity}}},
  shorttitle = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  year = {2022},
  month = may,
  pages = {8086--8098},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.556},
  url= {https://aclanthology.org/2022.acl-long.556},
  urldate = {2022-06-15}
}


@misc{OpenAI_2023_GPT-4_Technical_Report,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  url={http://arxiv.org/abs/2303.08774},    
  urldate = {2023-06-18},
  archiveprefix = {arXiv}
}

@misc{Smith_2021_13400_Artists_Out_of_7_Million_Earn_$50k_or_More_From_Spotify_Yearly,
  title = {13,400 {{Artists}} ({{Out}} of 7 {{Million}}) {{Earn}} \$50k or {{More From Spotify Yearly}}},
  author = {Smith, Dylan},
  year = {2021},
  month = mar,
  journal = {Digital Music News},
  url = {https://www.digitalmusicnews.com/2021/03/18/spotify-artist-earnings-figures/},
  urldate = {2022-10-19},
  abstract = {Just 13,400 artists, or about .19 percent of on-platform musicians, earn \$50,000 or more per year from Spotify royalties, new figures reveal.},
  langid = {american}
}

@misc{CarliniIppolitoEtAl_2022_Quantifying_Memorization_Across_Neural_Language_Models,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = arXiv,
  doi = {10.48550/arXiv.2202.07646},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv}
}

@misc{JosephSaveriLawFirmButterick_2022_GitHub_Copilot_investigation,
  title = {{{GitHub Copilot}} Investigation},
  author = {{Joseph Saveri Law Firm} and Butterick, Matthew},
  year = {2022},
  url = {https://www.saverilawfirm.com/our-cases/github-copilot-intellectual-property-litigation},
  urldate = {2022-10-18},
  abstract = {GitHub Copilot investigation}
}

@unpublished{TaylorKardasEtAl_2022_Galactica_Large_Language_Model_for_Science,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic},
  year = {2022},
  url = {https://galactica.org/static/paper.pdf}
}

@misc{CarliniIppolitoEtAl_2022_Quantifying_Memorization_Across_Neural_Language_Models,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = arXiv,
  doi = {10.48550/arXiv.2202.07646},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv}
}

@inproceedings{CarliniTramerEtAl_2021_Extracting_Training_Data_from_Large_Language_Models,
  title = {Extracting {{Training Data}} from {{Large Language Models}}},
  booktitle = {30th {{USENIX Security Symposium}} ({{USENIX Security}} 21)},
  author = {Carlini, Nicholas and Tram{\`e}r, Florian and Wallace, Eric and Jagielski, Matthew and {Herbert-Voss}, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, {\'U}lfar and Oprea, Alina and Raffel, Colin},
  year = {2021},
  pages = {2633--2650},
  url = {https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting},
  urldate = {2022-10-19},
  isbn = {978-1-939133-24-3},
  langid = {english}
}

@misc{LeeLeEtAl_2022_Do_Language_Models_Plagiarize,
  title = {Do {{Language Models Plagiarize}}?},
  author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07618},
  eprint = {2203.07618},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = arXiv,
  doi = {10.48550/arXiv.2203.07618},
  abstract = {Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.},
  archiveprefix = {arXiv}
}

@article{MetzlerTayEtAl_2021_Rethinking_search_making_domain_experts_out_of_dilettantes,
  title = {Rethinking Search: Making Domain Experts out of Dilettantes},
  shorttitle = {Rethinking Search},
  author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
  year = {2021},
  month = jul,
  journal = {ACM SIGIR Forum},
  volume = {55},
  number = {1},
  pages = {13:1--13:27},
  issn = {0163-5840},
  doi = {10.1145/3476415.3476428},
  abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.}
}

@inproceedings{ShahBender_2022_Situating_Search,
  title = {Situating {{Search}}},
  booktitle = {{{ACM SIGIR Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Shah, Chirag and Bender, Emily M.},
  year = {2022},
  month = mar,
  series = {{{CHIIR}} '22},
  pages = {221--232},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3498366.3505816},
  abstract = {Search systems, like many other applications of machine learning, have become increasingly complex and opaque. The notions of relevance, usefulness, and trustworthiness with respect to information were already overloaded and often difficult to articulate, study, or implement. Newly surfaced proposals that aim to use large language models to generate relevant information for a user's needs pose even greater threat to transparency, provenance, and user interactions in a search system. In this perspective paper we revisit the problem of search in the larger context of information seeking and argue that removing or reducing interactions in an effort to retrieve presumably more relevant information can be detrimental to many fundamental aspects of search, including information verification, information literacy, and serendipity. In addition to providing suggestions for counteracting some of the potential problems posed by such models, we present a vision for search systems that are intelligent and effective, while also providing greater transparency and accountability.},
  isbn = {978-1-4503-9186-3}
}


@misc{Heikkila_2022_This_artist_is_dominating_AI-generated_art_And_hes_not_happy_about_it,
  title = {This Artist Is Dominating {{AI-generated}} Art. {{And}} He's Not Happy about It.},
  author = {Heikkil{\"a}, Melissa},
  year = {2022},
  month = sep,
  journal = {MIT Technology Review},
  url = {https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/},
  urldate = {2022-10-18},
  abstract = {Greg Rutkowski is a more popular prompt than Picasso.},
  langid = {english}
}


@article{HitsuwariUedaEtAl_2022_Does_human-AI_collaboration_lead_to_more_creative_art,
  title = {Does Human\textendash{{AI}} Collaboration Lead to More Creative Art? {{Aesthetic}} Evaluation of Human-Made and {{AI-generated}} Haiku Poetry},
  shorttitle = {Does Human\textendash{{AI}} Collaboration Lead to More Creative Art?},
  author = {Hitsuwari, Jimpei and Ueda, Yoshiyuki and Yun, Woojin and Nomura, Michio},
  year = {2022},
  month = oct,
  journal = {Computers in Human Behavior},
  pages = {107502},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2022.107502},
  abstract = {With the development of technology, the quality of AI-generated text has improved. This is relevant in the AI art field, where AI generates literature or poetry that is appreciated. This study compared human-made and AI-generated haiku poetry, which is composed with 17 syllables and the world's shortest and clearest rules, to examine aesthetic evaluations of AI art and people's beliefs about it. AI-generated haiku were divided into those with and without human intervention. Two tasks were completed by 385 participants. The first involved evaluating human-made and AI-generated haiku on 21 items, such as beauty. The second involved determining whether the haiku were human-made or AI-generated. The results showed that the beauty rating of the AI-generated haiku with the human intervention was the highest, and those of the human-made and AI-generated haiku without human intervention were equal. Furthermore, participants could not distinguish between human-made and AI-generated haiku. These results suggest that human\textendash AI collaboration has better creativity in haiku production. Moreover, a negative correlation was found between discrimination performance and beauty rating in AI-generated haiku, suggesting that high-quality AI-generated work is believed to be human-made. This study indicates the potential of human\textendash AI collaboration in haiku and the underestimation of AI art due to algorithm aversion.},
  langid = {english}
}

@incollection{Bakhtin_1981_Discourse_in_novel,
  title = {Discourse in the Novel},
  booktitle = {The {{Dialogic}} Imagination: Four Essays},
  author = {Bakhtin, M. M.},
  editor = {Holquist, M.},
  year = {1981},
  publisher = {{University of Texas Press}},
  annotation = {Open Library ID: OL20720399M}
}

@incollection{Barthes_1977_Death_of_Author,
  title = {The {{Death}} of the {{Author}}},
  booktitle = {Image, Music, Text: Essays},
  author = {Barthes, Roland},
  translator = {Heath, Stephen},
  year = {1977},
  edition = {Thirteenth},
  pages = {142--148},
  publisher = Fontana,
  address = {{London}},
  url = {https://sites.tufts.edu/english292b/files/2012/01/Barthes-The-Death-of-the-Author.pdf},
  isbn = {978-0-00-686135-5},
  langid = {english}
}

@book{Kristeva_1980_Desire_in_language_semiotic_approach_to_literature_and_art,
  title = {Desire in language: A semiotic approach to literature and art},
  author = {Kristeva, Julia},
  year = {1980},
  publisher = {Columbia University Press}
}


@book{Ahrens_2017_How_to_take_smart_notes_one_simple_technique_to_boost_writing_learning_and_thinking_for_students_academics_and_nonfiction_book_writers,
  address = {{North Charleston, SC}},
  title = {How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking: For Students, Academics and Nonfiction Book Writers},
  isbn = {978-1-5428-6650-7},
  shorttitle = {How to Take Smart Notes},
  language = {eng},
  publisher = {{CreateSpace}},
  author = {Ahrens, S{\"o}nke},
  year = {2017}
}


@book{Kahneman_2013_Thinking_fast_and_slow,
  address = {{New York}},
  edition = {1st pbk. ed},
  title = {Thinking, Fast and Slow},
  isbn = {978-0-374-53355-7},
  lccn = {BF441 .K238 2013},
  abstract = {In this work the author, a recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology that challenged the rational model of judgment and decision making, has brought together his many years of research and thinking in one book. He explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. He exposes the extraordinary capabilities, and also the faults and biases, of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. He reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives, and how we can use different techniques to guard against the mental glitches that often get us into trouble. This author's work has transformed cognitive psychology and launched the new fields of behavioral economics and happiness studies. In this book, he takes us on a tour of the mind and explains the two systems that drive the way we think and the way we make choices},
  publisher = {{Farrar, Straus and Giroux}},
  author = {Kahneman, Daniel},
  year = {2013}
}


@inproceedings{GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data,
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
  year = {2018},
  pages = {107--112},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-2017},
  url = {https://www.aclweb.org/anthology/N18-2017}
}




@article{Crane_2018_Questionable_Answers_in_Question_Answering_Research_Reproducibility_and_Variability_of_Published_Results,
  title = {Questionable {{Answers}} in {{Question Answering Research}}: {{Reproducibility}} and {{Variability}} of {{Published Results}}},
  volume = {6},
  shorttitle = {Questionable {{Answers}} in {{Question Answering Research}}},
  language = {en-us},
  journal = {Transactions of the Association for Computational Linguistics},
  doi = {10.1162/tacl_a_00018},
  url = {https://aclweb.org/anthology/papers/Q/Q18/Q18-1018/},
  author = {Crane, Matt},
  year = {2018},
  pages = {241-252}
}


@inproceedings{EscartinReijersEtAl_2017_Ethical_Considerations_in_NLP_Shared_Tasks,
  title = {Ethical {{Considerations}} in {{NLP Shared Tasks}}},
  language = {en-us},
  booktitle = {Proceedings of the {{First ACL Workshop}} on {{Ethics}} in {{Natural Language Processing}}},
  doi = {10.18653/v1/W17-1608},
  url = {https://aclweb.org/anthology/papers/W/W17/W17-1608/},
  author = {Escart{\'i}n, Carla Parra and Reijers, Wessel and Lynn, Teresa and Moorkens, Joss and Way, Andy and Liu, Chao-Hong},
  month = apr,
  year = {2017},
  pages = {66-73}
}


@inproceedings{SugawaraStenetorpEtAl_2020_Assessing_Benchmarking_Capacity_of_Machine_Reading_Comprehension_Datasets,
  title = {Assessing the {{Benchmarking Capacity}} of {{Machine Reading Comprehension Datasets}}},
  booktitle = {{{AAAI}}},
  author = {Sugawara, Saku and Stenetorp, Pontus and Inui, Kentaro and Aizawa, Akiko},
  year = {2020},
  url = {http://arxiv.org/abs/1911.09241},
  archivePrefix = {arXiv}
}


@inproceedings{JiaLiang_2017_Adversarial_Examples_for_Evaluating_Reading_Comprehension_Systems,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2017},
  pages = {2021--2031},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1215},
  url = {http://aclweb.org/anthology/D17-1215}
}


@inproceedings{Yatskar_2019_Qualitative_Comparison_of_CoQA_SQuAD_20_and_QuAC,
  title = {A {{Qualitative Comparison}} of {{CoQA}}, {{SQuAD}} 2.0 and {{QuAC}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Yatskar, Mark},
  year = {2019},
  pages = {2318--2323},
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1241/}
}


@article{RogersKovalevaEtAl_2020_Primer_in_BERTology_What_we_know_about_how_BERT_works,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  url = {http://arxiv.org/abs/2002.12327},
  archivePrefix = {arXiv},
  journal = {arXiv:2002.12327 [cs]},
  primaryClass = {cs}
}


@inproceedings{RogersRomanovEtAl_2018_RuSentiment_Enriched_Sentiment_Analysis_Dataset_for_Social_Media_in_Russian,
  ids = {RogersRomanovEtAl\_2018\_RuSentiment\_An\_Enriched\_Sentiment\_Analysis\_Dataset\_for\_Social\_Media\_in\_Russian},
  title = {{{RuSentiment}}: {{An Enriched Sentiment Analysis Dataset}} for {{Social Media}} in {{Russian}}},
  shorttitle = {{{RuSentiment}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Rogers, Anna and Romanov, Alexey and Rumshisky, Anna and Volkova, Svitlana and Gronas, Mikhail and Gribov, Alex},
  year = {2018},
  pages = {755--763},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico, USA}},
  url = {http://aclweb.org/anthology/C18-1064}
}


@inproceedings{GevaGoldbergEtAl_2019_Are_We_Modeling_Task_or_Annotator_Investigation_of_Annotator_Bias_in_Natural_Language_Understanding_Datasets,
  title = {Are {{We Modeling}} the {{Task}} or the {{Annotator}}? {{An Investigation}} of {{Annotator Bias}} in {{Natural Language Understanding Datasets}}},
  shorttitle = {Are {{We Modeling}} the {{Task}} or the {{Annotator}}?},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Geva, Mor and Goldberg, Yoav and Berant, Jonathan},
  year = {2019},
  pages = {1161--1166},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1107},
  url = {https://www.aclweb.org/anthology/D19-1107}
}




@inproceedings{SinghMcCannEtAl_2019_BERT_is_Not_Interlingua_and_Bias_of_Tokenization,
  title = {{{BERT}} Is {{Not}} an {{Interlingua}} and the {{Bias}} of {{Tokenization}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Deep Learning Approaches}} for {{Low}}-{{Resource NLP}} ({{DeepLo}} 2019)},
  author = {Singh, Jasdeep and McCann, Bryan and Socher, Richard and Xiong, Caiming},
  year = {2019},
  pages = {47--55},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-6106},
  url = {https://www.aclweb.org/anthology/D19-6106}
}


@article{NivreAgicEtAl_2015_Universal_Dependencies_12,
  title = {Universal Dependencies 1.2},
  author = {Nivre, Joakim and Agić, Željko and Aranzabe, Maria Jesus and Asahara, Masayuki and Atutxa, Aitziber and Ballesteros, Miguel and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bosco, Cristina and Bowman, Sam and Celano, Giuseppe G. A. and Connor, Miriam and de Marneffe, Marie-Catherine and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Erjavec, Tomaž and Farkas, Richárd and Foster, Jennifer and Galbraith, Daniel and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Goldberg, Yoav and Gonzales, Berta and Guillaume, Bruno and Hajič, Jan and Haug, Dag and Ion, Radu and Irimia, Elena and Johannsen, Anders and Kanayama, Hiroshi and Kanerva, Jenna and Krek, Simon and Laippala, Veronika and Lenci, Alessandro and Ljubešić, Nikola and Lynn, Teresa and Manning, Christopher and Mărănduc, Cătălina and Mareček, David and Martínez Alonso, Héctor and Mašek, Jan and Matsumoto, Yuji and McDonald, Ryan and Missilä, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and Mori, Shunsuke and Nurmi, Hanna and Osenova, Petya and Øvrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Petrov, Slav and Piitulainen, Jussi and Plank, Barbara and Popel, Martin and Prokopidis, Prokopis and Pyysalo, Sampo and Ramasamy, Loganathan and Rosa, Rudolf and Saleh, Shadi and Schuster, Sebastian and Seeker, Wolfgang and Seraji, Mojgan and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simkó, Katalin and Simov, Kiril and Smith, Aaron and Štěpánek, Jan and Suhr, Alane and Szántó, Zsolt and Tanaka, Takaaki and Tsarfaty, Reut and Uematsu, Sumire and Uria, Larraitz and Varga, Viktor and Vincze, Veronika and Žabokrtský, Zdeněk and Zeman, Daniel and Zhu, Hanzhi},
  year = {2015},
  url = {http://hdl.handle.net/11234/1-1548},
  journal = {LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University}
}


@misc{bawden2019diabla,
    title={DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation},
    author={Rachel Bawden and Sophie Rosset and Thomas Lavergne and Eric Bilinski},
    year={2019},
    eprint={1905.13354},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}







@inproceedings{McCoyPavlickEtAl_2019_Right_for_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  url = {https://www.aclweb.org/anthology/P19-1334}
}




@inproceedings{DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1423/},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = jun,
  year = {2019},
  pages = {4171-4186}
}



@article{YangDaiEtAl_2019_XLNet_Generalized_Autoregressive_Pretraining_for_Language_Understanding,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.08237},
  primaryClass = {cs},
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
  journal = {arXiv:1906.08237 [cs]},
  url = {http://arxiv.org/abs/1906.08237},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  month = jun,
  year = {2019}
}



@inproceedings{PetersNeumannEtAl_2018_Deep_Contextualized_Word_Representations,
  title = {Deep {{Contextualized Word Representations}}},
  language = {en-us},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  doi = {10.18653/v1/N18-1202},
  url = {https://aclweb.org/anthology/papers/N/N18/N18-1202/},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  month = jun,
  year = {2018},
  pages = {2227-2237}
}


@inproceedings{ZhangHanEtAl_2019_ERNIE_Enhanced_Language_Representation_with_Informative_Entities,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.07129},
  title = {{{ERNIE}}: {{Enhanced Language Representation}} with {{Informative Entities}}},
  shorttitle = {{{ERNIE}}},
  abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.},
  booktitle = {{{ACL}} 2019},
  url = {http://arxiv.org/abs/1905.07129},
  author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  month = may,
  year = {2019}
}


@article{RadfordWuEtAl_2019_Language_models_are_unsupervised_multitask_learners,
  title = {Language Models Are Unsupervised Multitask Learners},
  volume = {1},
  journal = {OpenAI Blog},
  url = {https://openai.com/blog/better-language-models/},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  pages = {8}
}


@inproceedings{StrubellGaneshEtAl_2019_Energy_and_Policy_Considerations_for_Deep_Learning_in_NLP,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02243},
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  booktitle = {{{ACL}} 2019},
  url = {http://arxiv.org/abs/1906.02243},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  month = jun,
  year = {2019}
}


@inproceedings{WuFanEtAl_2019_Pay_Less_Attention_with_Lightweight_and_Dynamic_Convolutions,
  title = {Pay {{Less Attention}} with {{Lightweight}} and {{Dynamic Convolutions}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  url = {https://openreview.net/forum?id=SkVhlh09tX},
  author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
  year = {2019}
}


@inproceedings{FrankleCarbin_2019_Lottery_Ticket_Hypothesis_Finding_Sparse_Trainable_Neural_Networks,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  url = {https://openreview.net/forum?id=rJl-b3RcF7},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019}
}


@article{Goldberg_2019_Assessing_BERTs_Syntactic_Abilities,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.05287},
  primaryClass = {cs},
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
  journal = {arXiv:1901.05287 [cs]},
  url = {http://arxiv.org/abs/1901.05287},
  author = {Goldberg, Yoav},
  month = jan,
  year = {2019}
}


@inproceedings{LapesaEvert_2017_Large-scale_evaluation_of_dependency-based_DSMs_Are_they_worth_the_effort,
  title = {Large-Scale Evaluation of Dependency-Based {{DSMs}}: {{Are}} They Worth the Effort?},
  shorttitle = {Large-Scale Evaluation of Dependency-Based {{DSMs}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{EACL}})},
  publisher = {{Association for Computational Linguistics}},
  url = {http://www.aclweb.org/anthology/E17-2063},
  author = {Lapesa, Gabriella and Evert, Stefan},
  year = {2017},
  pages = {394-400}
}


@inproceedings{LiLiuEtAl_2017_Investigating_Different_Syntactic_Context_Types_and_Context_Representations_for_Learning_Word_Embeddings,
  address = {{Copenhagen, Denmark, September 7\textendash{}11, 2017}},
  title = {Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  url = {http://aclweb.org/anthology/D17-1257},
  author = {Li, Bofang and Liu, Tao and Zhao, Zhe and Tang, Buzhou and Drozd, Aleksandr and Rogers, Anna and Du, Xiaoyong},
  year = {2017},
  pages = {2411--2421}
}


@article{VoitaTalbotEtAl_2019_Analyzing_Multi-Head_Self-Attention_Specialized_Heads_Do_Heavy_Lifting_Rest_Can_Be_Pruned,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09418},
  primaryClass = {cs},
  title = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}: {{Specialized Heads Do}} the {{Heavy Lifting}}, the {{Rest Can Be Pruned}}},
  shorttitle = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}},
  abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
  journal = {arXiv:1905.09418 [cs]},
  url = {http://arxiv.org/abs/1905.09418},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  month = may,
  year = {2019}
}


@article{ClarkKhandelwalEtAl_2019_What_Does_BERT_Look_At_Analysis_of_BERTs_Attention,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.04341},
  primaryClass = {cs},
  title = {What {{Does BERT Look At}}? {{An Analysis}} of {{BERT}}'s {{Attention}}},
  shorttitle = {What {{Does BERT Look At}}?},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  journal = {arXiv:1906.04341 [cs]},
  url = {http://arxiv.org/abs/1906.04341},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  month = jun,
  year = {2019}
}


@article{LinTanEtAl_2019_Open_Sesame_Getting_Inside_BERTs_Linguistic_Knowledge,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01698},
  primaryClass = {cs},
  title = {Open {{Sesame}}: {{Getting Inside BERT}}'s {{Linguistic Knowledge}}},
  shorttitle = {Open {{Sesame}}},
  abstract = {How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT's representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.},
  journal = {arXiv:1906.01698 [cs]},
  url = {http://arxiv.org/abs/1906.01698},
  author = {Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  month = jun,
  year = {2019}
}

@inproceedings{JawaharSagotEtAl_What_does_BERT_learn_about_structure_of_language,
  title = {What Does {{BERT}} Learn about the Structure of Language?},
  abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We first show that BERT's phrasal representation captures phrase-level information in the lower layers. We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
  language = {en},
  booktitle = {{{ACL}} 2019},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  pages = {8}
}

@article{CoenenReifEtAl_2019_Visualizing_and_Measuring_Geometry_of_BERT,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02715},
  primaryClass = {cs, stat},
  title = {Visualizing and {{Measuring}} the {{Geometry}} of {{BERT}}},
  abstract = {Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
  journal = {arXiv:1906.02715 [cs, stat]},
  url = {http://arxiv.org/abs/1906.02715},
  author = {Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  month = jun,
  year = {2019}
}


@inproceedings{MikolovChenEtAl_2013_Efficient_estimation_of_word_representations_in_vector_space,
  title = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {Proceedings of {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  url = {https://arxiv.org/pdf/1301.3781},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
}


@inproceedings{MikolovYihEtAl_2013_Linguistic_Regularities_in_Continuous_Space_Word_Representations,
  address = {{Atlanta, Georgia, 9\textendash{}14 June 2013}},
  title = {Linguistic {{Regularities}} in {{Continuous Space Word Representations}}.},
  booktitle = {Proceedings of {{NAACL}}-{{HLT}} 2013},
  url = {https://www.aclweb.org/anthology/N13-1090},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  keywords = {_Sasha},
  pages = {746--751}
}


@inproceedings{KoperScheibleEtAl_2015_Multilingual_reliability_and_semantic_structure_of_continuous_word_spaces,
  title = {Multilingual Reliability and "Semantic" Structure of Continuous Word Spaces},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Computational Semantics}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://www.aclweb.org/anthology/W15-01\#page=56},
  author = {K{\"o}per, Maximilian and Scheible, Christian and {im Walde}, Sabine Schulte},
  year = {2015},
  pages = {40-45}
}


@inproceedings{VylomovaRimmelEtAl_2016_Take_and_took_gaggle_and_goose_book_and_read_evaluating_utility_of_vector_differences_for_lexical_relation_learning,
  address = {{Berlin, Germany}},
  title = {Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning},
  shorttitle = {Take and {{Took}}, {{Gaggle}} and {{Goose}}, {{Book}} and {{Read}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P16-1158},
  url = {http://www.aclweb.org/anthology/P16-1158},
  author = {Vylomova, Ekaterina and Rimmel, Laura and Cohn, Trevor and Baldwin, Timothy},
  year = {2016},
  keywords = {_Sasha},
  pages = {1671--1682}
}



@inproceedings{KarpinskaLiEtAl_2018_Subcharacter_Information_in_Japanese_Embeddings_When_Is_It_Worth_It,
  address = {{Melbourne, Australia}},
  title = {Subcharacter {{Information}} in {{Japanese Embeddings}}: {{When Is It Worth It}}?},
  booktitle = {Proceedings of the {{Workshop}} on the {{Relevance}} of {{Linguistic Structure}} in {{Neural Architectures}} for {{NLP}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/W18-2905},
  author = {Karpinska, Marzena and Li, Bofang and Rogers, Anna and Drozd, Aleksandr},
  year = {2018},
  pages = {28-37}
}


@inproceedings{GladkovaDrozdEtAl_2016_Analogybased_detection_of_morphological_and_semantic_relations_with_word_embeddings_what_works_and_what_doesnt,
  address = {{San Diego, California, June 12-17, 2016}},
  title = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings: What Works and What Doesn't.},
  booktitle = {Proceedings of the {{NAACL}}-{{HLT SRW}}},
  publisher = {{ACL}},
  doi = {10.18653/v1/N16-2002},
  url = {https://www.aclweb.org/anthology/N/N16/N16-2002.pdf},
  author = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
  year = {2016},
  keywords = {peer-reviewed},
  pages = {47-54}
}


@inproceedings{Linzen_2016_Issues_in_evaluating_semantic_spaces_using_word_analogies,
  title = {Issues in Evaluating Semantic Spaces Using Word Analogies.},
  booktitle = {Proceedings of the {{First Workshop}} on {{Evaluating Vector Space Representations}} for {{NLP}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {http://dx.doi.org/10.18653/v1/W16-2503},
  url = {http://anthology.aclweb.org/W16-2503},
  author = {Linzen, Tal},
  year = {2016}
}


@inproceedings{GittensAchlioptasEtAl_2017_SkipGram_Zipf_Uniform_Vector_Additivity,
  title = {Skip-{{Gram}} - {{Zipf}} + {{Uniform}} = {{Vector Additivity}}},
  language = {en-us},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  doi = {10.18653/v1/P17-1007},
  url = {https://www.aclweb.org/anthology/papers/P/P17/P17-1007/},
  author = {Gittens, Alex and Achlioptas, Dimitris and Mahoney, Michael W.},
  month = jul,
  year = {2017},
  pages = {69-76}
}

@article{EthayarajhDuvenaudEtAl_2019_Towards_Understanding_Linear_Word_Analogies,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04882},
  title = {Towards {{Understanding Linear Word Analogies}}},
  abstract = {A surprising property of word vectors is that word analogies can often be solved with vector arithmetic. However, it is unclear why arithmetic operators correspond to non-linear embedding models such as skip-gram with negative sampling (SGNS). We provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution. Our theory has several implications. Past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios; we prove that this holds for SGNS. We provide novel justification for the addition of SGNS word vectors by showing that it automatically down-weights the more frequent word, as weighting schemes do ad hoc. Lastly, we offer an information theoretic interpretation of Euclidean distance in vector spaces, justifying its use in capturing word dissimilarity.},
  journal = {To appear in ACL 2019},
  url = {http://arxiv.org/abs/1810.04882},
  author = {Ethayarajh, Kawin and Duvenaud, David and Hirst, Graeme},
  month = oct,
  year = {2019}
}


@inproceedings{Schluter_2018_Word_Analogy_Testing_Caveat,
  title = {The {{Word Analogy Testing Caveat}}},
  language = {en-us},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  doi = {10.18653/v1/N18-2039},
  url = {https://www.aclweb.org/anthology/papers/N/N18/N18-2039/},
  author = {Schluter, Natalie},
  month = jun,
  year = {2018},
  pages = {242-246}
}



@inproceedings{WashioKato_2018_Neural_Latent_Relational_Analysis_to_Capture_Lexical_Semantic_Relations_in_a_Vector_Space,
  address = {{Brussels, Belgium}},
  title = {Neural {{Latent Relational Analysis}} to {{Capture Lexical Semantic Relations}} in a {{Vector Space}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/D18-1058},
  author = {Washio, Koki and Kato, Tsuneaki},
  year = {2018},
  pages = {594-600}
}

@article{JoshiChoiEtAl_2018_pair2vec_Compositional_Word-Pair_Embeddings_for_Cross-Sentence_Inference,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.08854},
  primaryClass = {cs},
  title = {Pair2vec: {{Compositional Word}}-{{Pair Embeddings}} for {{Cross}}-{{Sentence Inference}}},
  shorttitle = {Pair2vec},
  abstract = {Reasoning about implied relationships (e.g. paraphrastic, common sense, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function of each word's representation, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the the two words co-occur. We add these representations to the cross-sentence attention layer of existing inference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or replacing existing word embeddings. Experiments show a gain of 2.72\% on the recently released SQuAD 2.0 and 1.3\% on MultiNLI. Our representations also aid in better generalization with gains of around 6-7\% on adversarial SQuAD datasets, and 8.8\% on the adversarial entailment test set by Glockner et al.},
  journal = {arXiv:1810.08854 [cs]},
  url = {http://arxiv.org/abs/1810.08854},
  author = {Joshi, Mandar and Choi, Eunsol and Levy, Omer and Weld, Daniel S. and Zettlemoyer, Luke},
  month = oct,
  year = {2018}
}

@article{Camacho-ColladosEspinosa-AnkeEtAl_2019_Relational_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.01373},
  title = {Relational {{Word Embeddings}}},
  abstract = {While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.},
  journal = {ACL 2019},
  url = {http://arxiv.org/abs/1906.01373},
  author = {{Camacho-Collados}, Jose and {Espinosa-Anke}, Luis and Schockaert, Steven},
  month = jun,
  year = {2019}
}


@inproceedings{HakamiHayashiEtAl_2018_Why_does_PairDiff_work,
  title = {Why Does {{PairDiff}} Work? - {{A Mathematical Analysis}} of {{Bilinear Relational Compositional Operators}} for {{Analogy Detection}}},
  shorttitle = {Why Does {{PairDiff}} Work?},
  language = {en-us},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  url = {https://www.aclweb.org/anthology/papers/C/C18/C18-1211/},
  author = {Hakami, Huda and Hayashi, Kohei and Bollegala, Danushka},
  month = aug,
  year = {2018},
  pages = {2493-2504}
}


@inproceedings{DrozdGladkovaEtAl_2016_Word_embeddings_analogies_and_machine_learning_beyond_king_man_woman_queen,
  address = {{Osaka, Japan, December 11-17}},
  title = {Word Embeddings, Analogies, and Machine Learning: Beyond King - Man + Woman = Queen},
  shorttitle = {Word {{Embeddings}}, {{Analogies}}, and {{Machine Learning}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  url = {https://www.aclweb.org/anthology/C/C16/C16-1332.pdf},
  author = {Drozd, Aleksandr and Gladkova, Anna and Matsuoka, Satoshi},
  year = {2016},
  keywords = {peer-reviewed},
  pages = {3519--3530}
}


@article{NissimvanNoordEtAl_2019_Fair_is_Better_than_SensationalMan_is_to_Doctor_as_Woman_is_to_Doctor,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09866},
  primaryClass = {cs},
  title = {Fair Is {{Better}} than {{Sensational}}:{{Man}} Is to {{Doctor}} as {{Woman}} Is to {{Doctor}}},
  shorttitle = {Fair Is {{Better}} than {{Sensational}}},
  abstract = {Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also exposed how strongly human biases are encoded in vector spaces built on natural language. While finding that queen is the answer to man is to king as woman is to X leaves us in awe, papers have also reported finding analogies deeply infused with human biases, like man is to computer programmer as woman is to homemaker, which instead leave us with worry and rage. In this work we show that,often unknowingly, embedding spaces have not been treated fairly. Through a series of simple experiments, we highlight practical and theoretical problems in previous works, and demonstrate that some of the most widely used biased analogies are in fact not supported by the data. We claim that rather than striving to find sensational biases, we should aim at observing the data "as is", which is biased enough. This should serve as a fair starting point to properly address the evident, serious, and compelling problem of human bias in word embeddings.},
  journal = {arXiv:1905.09866 [cs]},
  url = {http://arxiv.org/abs/1905.09866},
  author = {Nissim, Malvina and {van Noord}, Rik and {van der Goot}, Rob},
  month = may,
  year = {2019}
}


@inproceedings{BolukbasiChangEtAl_2016_Man_is_to_Computer_Programmer_As_Woman_is_to_Homemaker_Debiasing_Word_Embeddings,
  address = {{USA}},
  series = {{{NIPS}}'16},
  title = {Man Is to {{Computer Programmer As Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  isbn = {978-1-5108-3881-9},
  shorttitle = {Man Is to {{Computer Programmer As Woman}} Is to {{Homemaker}}?},
  abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Curran Associates Inc.}},
  url = {http://dl.acm.org/citation.cfm?id=3157382.3157584},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
  year = {2016},
  pages = {4356--4364}
}


@inproceedings{RogersDrozdEtAl_2017_Too_Many_Problems_of_Analogical_Reasoning_with_Word_Vectors,
  title = {The ({{Too Many}}) {{Problems}} of {{Analogical Reasoning}} with {{Word Vectors}}},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (* {{SEM}} 2017)},
  url = {http://www.aclweb.org/anthology/S17-1017},
  author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
  year = {2017},
  keywords = {peer-reviewed},
  pages = {135--148}
}


@inproceedings{ThomasonGordonEtAl_2019_Shifting_Baseline_Single_Modality_Performance_on_Visual_Navigation_QA,
  title = {Shifting the {{Baseline}}: {{Single Modality Performance}} on {{Visual Navigation}} \& {{QA}}},
  shorttitle = {Shifting the {{Baseline}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1197/},
  author = {Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan},
  month = jun,
  year = {2019},
  pages = {1977-1983}
}


@inproceedings{Yatskar_2019_Qualitative_Comparison_of_CoQA_SQuAD_20_and_QuAC,
  title = {A {{Qualitative Comparison}} of {{CoQA}}, {{SQuAD}} 2.0 and {{QuAC}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1241/},
  author = {Yatskar, Mark},
  month = jun,
  year = {2019},
  pages = {2318-2323}
}


@inproceedings{JainWallace_2019_Attention_is_not_Explanation,
  title = {Attention Is Not {{Explanation}}},
  language = {en-us},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1357/},
  author = {Jain, Sarthak and Wallace, Byron C.},
  month = jun,
  year = {2019},
  pages = {3543-3556}
}


@inproceedings{VineGevaEtAl_2018_Unsupervised_Mining_of_Analogical_Frames_by_Constraint_Satisfaction,
  title = {Unsupervised {{Mining}} of {{Analogical Frames}} by {{Constraint Satisfaction}}},
  language = {en-us},
  booktitle = {Proceedings of the {{Australasian Language Technology Association Workshop}} 2018},
  url = {https://www.aclweb.org/anthology/papers/U/U18/U18-1004/},
  author = {Vine, Lance De and Geva, Shlomo and Bruza, Peter},
  month = dec,
  year = {2018},
  pages = {34-43}
}

@inproceedings{BouraouiJameelEtAl_2018_Relation_Induction_in_Word_Embeddings_Revisited,
  title = {Relation {{Induction}} in {{Word Embeddings Revisited}}},
  language = {en-us},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  url = {https://www.aclweb.org/anthology/papers/C/C18/C18-1138/},
  author = {Bouraoui, Zied and Jameel, Shoaib and Schockaert, Steven},
  month = aug,
  year = {2018},
  pages = {1627-1637}
}

@article{DufterSchutze_2019_Analytical_Methods_for_Interpretable_Ultradense_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.08654},
  primaryClass = {cs},
  title = {Analytical {{Methods}} for {{Interpretable Ultradense Word Embeddings}}},
  abstract = {Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. While DensRay is very closely related to the Densifier, it can be computed in closed form, is hyperparameter-free and thus more robust than the Densifier. We evaluate the methods on lexicon induction and set-based word analogy and conclude that analytical methods such as DensRay and SVMs are preferable. For word analogy we propose a new method to solve the task which outperforms the previous state of the art by large margins.},
  journal = {arXiv:1904.08654 [cs]},
  url = {http://arxiv.org/abs/1904.08654},
  author = {Dufter, Philipp and Sch{\"u}tze, Hinrich},
  month = apr,
  year = {2019}
}


@inproceedings{HakamiBollegala_2019_Learning_Relation_Representations_from_Word_Representations,
  title = {Learning {{Relation Representations}} from {{Word Representations}}},
  abstract = {Identifying the relations that connect words is an important step towards understanding human languages and is useful for various NLP tasks such as knowledge base completion and analogical...},
  booktitle = {{{AKBC}} 2019},
  url = {https://openreview.net/forum?id=r1e3WW5aTX\&noteId=BklR5HOySN},
  author = {Hakami, Huda and Bollegala, Danushka},
  year = {2019}
}



@article{AllenHospedales_2019_Analogies_Explained_Towards_Understanding_Word_Embeddings,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.09813},
  primaryClass = {cs, stat},
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  shorttitle = {Analogies {{Explained}}},
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of "\$w\_x\$ is to \$w\_y\$". From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
  journal = {arXiv:1901.09813 [cs, stat]},
  url = {http://arxiv.org/abs/1901.09813},
  author = {Allen, Carl and Hospedales, Timothy},
  month = jan,
  year = {2019}
}



@misc{Greene_2019_How_Twitter_shapes_global_public_conversation_Jack_Dorsey_speaks_at_TED2019,
  title = {How {{Twitter}} Shapes Global Public Conversation: {{Jack Dorsey}} Speaks at {{TED2019}}},
  shorttitle = {How {{Twitter}} Shapes Global Public Conversation},
  abstract = {At TED2019, head of TED Chris Anderson and TED current affairs curator Whitney Pennington Rodgers join Twitter and Square CEO Jack Dorsey to discuss the health of the global conversation and how Twitter could change what it incentivizes users to do -- moving away from outrage and mob behavior and towards productive, healthy conversation.},
  language = {en},
  journal = {TED Blog},
  url = {https://blog.ted.com/how-twitter-shapes-global-public-conversation-jack-dorsey-speaks-at-ted2019/},
  author = {Greene, Brown},
  month = apr,
  year = {2019}
}

@misc{Farokhmanesh_2018_What_remains_of_Chatroulette_its_gone_to_dongs,
  title = {What Remains of {{Chatroulette}}: It's Gone to the Do(n)Gs},
  shorttitle = {What Remains of {{Chatroulette}}},
  abstract = {A small community persists on the once popular site},
  journal = {The Verge},
  url = {https://www.theverge.com/2018/2/14/16381942/chatroulette-webcam-chat-male-community},
  author = {Farokhmanesh, Megan},
  month = feb,
  year = {2018}
}

@article{BozdagvandenHoven_2015_Breaking_filter_bubble_democracy_and_design,
  title = {Breaking the Filter Bubble: Democracy and Design},
  volume = {17},
  issn = {1572-8439},
  shorttitle = {Breaking the Filter Bubble},
  abstract = {It has been argued that the Internet and social media increase the number of available viewpoints, perspectives, ideas and opinions available, leading to a very diverse pool of information. However, critics have argued that algorithms used by search engines, social networking platforms and other large online intermediaries actually decrease information diversity by forming so-called ``filter bubbles''. This may form a serious threat to our democracies. In response to this threat others have developed algorithms and digital tools to combat filter bubbles. This paper first provides examples of different software designs that try to break filter bubbles. Secondly, we show how norms required by two democracy models dominate the tools that are developed to fight the filter bubbles, while norms of other models are completely missing in the tools. The paper in conclusion argues that democracy itself is a contested concept and points to a variety of norms. Designers of diversity enhancing tools must thus be exposed to diverse conceptions of democracy.},
  language = {en},
  number = {4},
  journal = {Ethics and Information Technology},
  doi = {10.1007/s10676-015-9380-y},
  url = {https://doi.org/10.1007/s10676-015-9380-y},
  author = {Bozdag, Engin and {van den Hoven}, Jeroen},
  month = dec,
  year = {2015},
  pages = {249-265}
}

@book{Pariser_2011_filter_bubble_what_Internet_is_hiding_from_you,
  address = {{New York, NY}},
  title = {The Filter Bubble: What the {{Internet}} Is Hiding from You},
  isbn = {978-1-59420-300-8 978-0-14-312123-7},
  shorttitle = {The Filter Bubble},
  abstract = {In December 2009, Google began customizing its search results for each user. Instead of giving you the most broadly popular result, Google now tries to predict what you are most likely to click on. According to MoveOn.org board president Eli Pariser, Google's change in policy is symptomatic of the most significant shift to take place on the Web in recent years-the rise of personalization. In this groundbreaking investigation of the new hidden Web, Pariser uncovers how this growing trend threatens to control how we consume and share information as a society-and reveals what we can do about it. Personalized filters are sweeping the Web, creating individual universes of information for each of us. Facebook prioritizes the links it believes will appeal to you so that if you are a liberal, you can expect to see only progressive links. Behind the scenes a burgeoning industry of data companies is tracking your personal information to sell to advertisers, from your political leanings to the color you painted your living room to the hiking boots you just browsed on Zappos. In a personalized world, we will increasingly be typed and fed only news that is pleasant, familiar, and confirms our beliefs-and because these filters are invisible, we won't know what is being hidden from us. Our past interests will determine what we are exposed to in the future, leaving less room for the unexpected encounters that spark creativity, innovation, and the democratic exchange of ideas. While we all worry that the Internet is eroding privacy or shrinking our attention spans, Pariser uncovers a more pernicious and far- reaching trend on the Internet and shows how we can- and must-change course. With vivid detail and remarkable scope, The Filter Bubble reveals how personalization undermines the Internet's original purpose as an open platform for the spread of ideas and could leave us all in an isolated, echoing world.},
  language = {English},
  publisher = {{Penguin Press}},
  author = {Pariser, Eli},
  year = {2011}
}

@misc{Keegan_Blue_Feed_Red_Feed,
  title = {Blue {{Feed}}, {{Red Feed}}},
  abstract = {See Liberal Facebook and Conservative Facebook, Side by Side},
  language = {en},
  journal = {WSJ},
  url = {http://graphics.wsj.com/blue-feed-red-feed/},
  author = {Keegan, Jon}
}

@misc{MacLellan_2016_scientifically_proven_step-by-step_guide_to_having_breakthrough_conversation_across_party_lines,
  title = {The Scientifically Proven, Step-by-Step Guide to Having a Breakthrough Conversation across Party Lines},
  abstract = {A conflict resolution strategy honed in the US civil rights era is back in vogue.},
  language = {en},
  journal = {Quartz},
  url = {https://qz.com/838321/nonviolent-communication-the-scientifically-proven-step-by-step-guide-to-having-a-breakthrough-conversation-across-party-lines/},
  author = {MacLellan, Lila},
  month = nov,
  year = {2016}
}

@misc{Adee_2016_How_can_Facebook_and_its_users_burst_filter_bubble,
  title = {How Can {{Facebook}} and Its Users Burst the 'Filter Bubble'?},
  abstract = {Social media filter bubbles have come under scrutiny following the US election. Design tweaks and new habits could help pop them and expand our views

\&nbsp;},
  language = {en-US},
  journal = {New Scientist},
  url = {https://www.newscientist.com/article/2113246-how-can-facebook-and-its-users-burst-the-filter-bubble/},
  author = {Adee, Sally},
  month = nov,
  year = {2016}
}

@misc{Pariser_2011_Beware_online_filter_bubbles,
  title = {Beware Online "Filter Bubbles"},
  abstract = {As web companies strive to tailor their services (including news and search results) to our personal tastes, there's a dangerous unintended consequence: We get trapped in a "filter bubble" and don't get exposed to information that could challenge or broaden our worldview. Eli Pariser argues powerfully that this will ultimately prove to be bad for us and bad for democracy.},
  language = {en},
  url = {https://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles},
  author = {Pariser, Eli},
  year = {2011}
}

@misc{Pachal_2016_How_to_improve_your_Facebook_feed_so_we_see_next_Trump_coming,
  title = {How to Improve Your {{Facebook}} Feed, so We See the next {{Trump}} Coming},
  abstract = {Time to pop the filter bubble.},
  language = {en},
  journal = {Mashable},
  url = {https://mashable.com/2016/11/17/avoid-facebook-filter-bubble/},
  author = {Pachal, Pete},
  month = nov,
  year = {2016}
}

@misc{Chen_2018_Cambridge_Analyticas_Facebook_data_abuse_shouldnt_get_credit_for_Trump,
  title = {Cambridge {{Analytica}}'s {{Facebook}} Data Abuse Shouldn't Get Credit for {{Trump}}},
  abstract = {"I think Cambridge Analytica is a better marketing company than a targeting company."},
  journal = {The Verge},
  url = {https://www.theverge.com/2018/3/20/17138854/cambridge-analytica-facebook-data-trump-campaign-psychographic-microtargeting},
  author = {Chen, Angela},
  month = mar,
  year = {2018}
}

@misc{Read_Across_Aisle,
  title = {Read {{Across The Aisle}}},
  abstract = {Ever wonder what your social media news feed isn't showing you? Escape your ``filter bubble'' with Read Across The Aisle, a new way to combat political polarization and stay informed!},
  language = {en},
  journal = {Read Across The Aisle},
  url = {www.readacrosstheaisle.com}
}

@misc{2019_AllSides_Balanced_news_via_media_bias_ratings_for_unbiased_news_perspective,
  title = {{{AllSides}} | {{Balanced}} News via Media Bias Ratings for an Unbiased News Perspective},
  abstract = {See issues and political news with news bias revealed. Non-partisan, crowd-sourced technology shows all sides so you can decide.},
  language = {en},
  journal = {AllSides},
  url = {https://www.allsides.com/unbiased-balanced-news},
  month = may,
  year = {2019}
}

@misc{Lumb_2015_Why_Scientists_Are_Upset_About_Facebook_Filter_Bubble_Study,
  title = {Why {{Scientists Are Upset About The Facebook Filter Bubble Study}}},
  abstract = {Facebook says users, not its algorithm, are responsible for the so-called filter bubble. But Facebook's own research says otherwise.},
  language = {en-US},
  journal = {Fast Company},
  url = {https://www.fastcompany.com/3046111/why-scientists-are-upset-over-the-facebook-filter-bubble-study},
  author = {Lumb, David},
  month = may,
  year = {2015}
}

@article{BakshyMessingEtAl_2015_Exposure_to_ideologically_diverse_news_and_opinion_on_Facebook,
  title = {Exposure to Ideologically Diverse News and Opinion on {{Facebook}}},
  volume = {348},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  abstract = {Not getting all sides of the news?
People are increasingly turning away from mass media to social media as a way of learning news and civic information. Bakshy et al. examined the news that millions of Facebook users' peers shared, what information these users were presented with, and what they ultimately consumed (see the Perspective by Lazer). Friends shared substantially less cross-cutting news from sources aligned with an opposing ideology. People encountered roughly 15\% less cross-cutting content in news feeds due to algorithmic ranking and clicked through to 70\% less of this cross-cutting content. Within the domain of political news encountered in social media, selective exposure appears to drive attention.
Science, this issue p. 1130; see also p. 1090
Exposure to news, opinion, and civic information increasingly occurs through social media. How do these online networks influence exposure to perspectives that cut across ideological lines? Using deidentified data, we examined how 10.1 million U.S. Facebook users interact with socially shared news. We directly measured ideological homophily in friend networks and examined the extent to which heterogeneous friends could potentially expose individuals to cross-cutting content. We then quantified the extent to which individuals encounter comparatively more or less diverse content while interacting via Facebook's algorithmically ranked News Feed and further studied users' choices to click through to ideologically discordant content. Compared with algorithmic ranking, individuals' choices played a stronger role in limiting exposure to cross-cutting content.
Despite the diversity of information available, people still pay attention to a limited range of opinions. [Also see Perspective by Lazer]
Despite the diversity of information available, people still pay attention to a limited range of opinions. [Also see Perspective by Lazer]},
  language = {en},
  number = {6239},
  journal = {Science},
  doi = {10.1126/science.aaa1160},
  url = {https://science.sciencemag.org/content/348/6239/1130},
  author = {Bakshy, Eytan and Messing, Solomon and Adamic, Lada A.},
  month = jun,
  year = {2015},
  pages = {1130-1132},
  pmid = {25953820}
}

@misc{Tam_2018_Pop_Goes_Filter_Bubble,
  title = {Pop {{Goes}} the {{Filter Bubble}}?},
  abstract = {Pop goes the filter bubble?},
  language = {en},
  journal = {Slate Magazine},
  url = {https://slate.com/technology/2018/04/personalization-algorithms-could-actually-help-pop-filter-bubbles.html},
  author = {Tam, Stephanie},
  month = apr,
  year = {2018}
}

@article{DeCarbonnel_2019_Exclusive_Echo_chambers_-_Fake_news_fact-checks_hobbled_by_low,
  title = {Exclusive: {{Echo}} Chambers - {{Fake}} News Fact-Checks Hobbled by Low...},
  shorttitle = {Exclusive},
  abstract = {The European Union has called on Facebook and other platforms to invest more in ...},
  language = {en},
  journal = {Reuters},
  url = {https://www.reuters.com/article/us-eu-disinformation-exclusive-idUSKCN1U60PT},
  author = {De Carbonnel, Alissa},
  month = jul,
  year = {2019}
}


@inproceedings{GillaniYuanEtAl_2018_Me_My_Echo_Chamber_and_I_Introspection_on_Social_Media_Polarization,
  address = {{Republic and Canton of Geneva, Switzerland}},
  series = {{{WWW}} '18},
  title = {Me, {{My Echo Chamber}}, and {{I}}: {{Introspection}} on {{Social Media Polarization}}},
  isbn = {978-1-4503-5639-8},
  shorttitle = {Me, {{My Echo Chamber}}, and {{I}}},
  abstract = {Homophily - our tendency to surround ourselves with others who share our perspectives and opinions about the world - is both a part of human nature and an organizing principle underpinning many of our digital social networks. However, when it comes to politics or culture, homophily can amplify tribal mindsets and produce "echo chambers" that degrade the quality, safety, and diversity of discourse online. While several studies have empirically proven this point, few have explored how making users aware of the extent and nature of their political echo chambers influences their subsequent beliefs and actions. In this paper, we introduce Social Mirror, a social network visualization tool that enables a sample of Twitter users to explore the politically-active parts of their social network. We use Social Mirror to recruit Twitter users with a prior history of political discourse to a randomized experiment where we evaluate the effects of different treatments on participants' i) beliefs about their network connections, ii) the political diversity of who they choose to follow, and iii) the political alignment of the URLs they choose to share. While we see no effects on average political alignment of shared URLs, we find that recommending accounts of the opposite political ideology to follow reduces participants\guillemotright{} beliefs in the political homogeneity of their network connections but still enhances their connection diversity one week after treatment. Conversely, participants who enhance their belief in the political homogeneity of their Twitter connections have less diverse network connections 2-3 weeks after treatment. We explore the implications of these disconnects between beliefs and actions on future efforts to promote healthier exchanges in our digital public spheres.},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  doi = {10.1145/3178876.3186130},
  url = {https://doi.org/10.1145/3178876.3186130},
  author = {Gillani, Nabeel and Yuan, Ann and Saveski, Martin and Vosoughi, Soroush and Roy, Deb},
  year = {2018},
  pages = {823--831}
}


@article{DodgeIlharcoEtAl_2020_Fine-Tuning_Pretrained_Language_Models_Weight_Initializations_Data_Orders_and_Early_Stopping,
  title = {Fine-{{Tuning Pretrained Language Models}}: {{Weight Initializations}}, {{Data Orders}}, and {{Early Stopping}}},
  shorttitle = {Fine-{{Tuning Pretrained Language Models}}},
  author = {Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  year = {2020},
  url = {http://arxiv.org/abs/2002.06305},
  archivePrefix = {arXiv},
  journal = {arXiv:2002.06305 [cs]},
  primaryClass = {cs}
}


@article{LiuOttEtAl_2019_RoBERTa_Robustly_Optimized_BERT_Pretraining_Approach,
  ids = {LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approach,LiuOttEtAl\_2019\_RoBERTa\_Robustly\_Optimized\_BERT\_Pretraining\_Approacha},
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  url = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.11692 [cs]},
  primaryClass = {cs}
}


@inproceedings{DodgeGururanganEtAl_2019_Show_Your_Work_Improved_Reporting_of_Experimental_Results,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  pages = {2185--2194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1224},
  url = {https://www.aclweb.org/anthology/D19-1224}
}


@inproceedings{DodgeGururanganEtAl_2019_Show_Your_Work_Improved_Reporting_of_Experimental_Results,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  pages = {2185--2194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1224},
  url = {https://www.aclweb.org/anthology/D19-1224}
}


@article{Anderson_2009_Conference_reviewing_considered_harmful,
  title = {Conference Reviewing Considered Harmful},
  author = {Anderson, Thomas},
  year = {2009},
  volume = {43},
  pages = {108},
  issn = {01635980},
  doi = {10.1145/1531793.1531815},
  url = {http://portal.acm.org/citation.cfm?doid=1531793.1531815},
  journal = {ACM SIGOPS Operating Systems Review},
  number = {2}
}





@article{SchwartzDodgeEtAl_2019_Green_AI,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  url = {http://arxiv.org/abs/1907.10597},
  archivePrefix = {arXiv},
  journal = {arXiv:1907.10597 [cs, stat]},
  primaryClass = {cs, stat}
}


@article{PetersCeci_1982_fate_of_published_articles_submitted_again,
  title = {The Fate of Published Articles, Submitted Again},
  author = {Peters, Douglas P. and Ceci, Stephen J.},
  year = {1982},
  month = jun,
  volume = {5},
  pages = {199--199},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00011213},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/fate-of-published-articles-submitted-again/17914E617A57CDF8ABF3D95F9F28E7FE},
  urldate = {2020-05-20},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0140525X00011213/resource/name/firstPage-S0140525X00011213a.jpg},
  journal = {Behavioral and Brain Sciences},
  language = {en},
  number = {2}
}



@article{BharadhwajTurpinEtAl_2020_De-anonymization_of_authors_through_arXiv_submissions_during_double-blind_review,
  title = {De-Anonymization of Authors through {{arXiv}} Submissions during Double-Blind Review},
  author = {Bharadhwaj, Homanga and Turpin, Dylan and Garg, Animesh and Anderson, Ashton},
  year = {2020},
  month = jun,
  url = {http://arxiv.org/abs/2007.00177},
  urldate = {2020-07-10},
  abstract = {In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website https://sites.google.com/view/deanon-arxiv/home},
  archivePrefix = {arXiv},
  eprint = {2007.00177},
  eprinttype = {arxiv},
  journal = {arXiv:2007.00177 [cs]},
  primaryClass = {cs}
}



@article{Church_2020_Emerging_trends_Reviewing_reviewers_again,
  title = {Emerging Trends: {{Reviewing}} the Reviewers (Again)},
  shorttitle = {Emerging Trends},
  author = {Church, Kenneth Ward},
  year = {2020},
  month = mar,
  volume = {26},
  pages = {245--257},
  publisher = {{Cambridge University Press}},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324920000030},
  url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-reviewing-the-reviewers-again/10CDC1D71E1AEB21456CFBDA187CBCB6},
  urldate = {2020-05-12},
  abstract = {The ACL-2019 Business meeting ended with a discussion of reviewing. Conferences are experiencing a success catastrophe. They are becoming bigger and bigger, which is not only a sign of success but also a challenge (for reviewing and more). Various proposals for reducing submissions were discussed at the Business meeting. IMHO, the problem is not so much too many submissions, but rather, random reviewing. We cannot afford to do reviewing as badly as we do (because that leads to even more submissions). Negative feedback loops are effective. The reviewing process will improve over time if reviewers teach authors how to write better submissions, and authors teach reviewers how to write more constructive reviews. If you have received a not-ok (unhelpful/offensive) review, please help program committees improve by sharing your not-ok reviews on social media.},
  journal = {Natural Language Engineering},
  language = {en},
  number = {2}
}


@article{RobertsVerhoef_2016_Double-blind_reviewing_at_EvoLang_11_reveals_gender_bias,
  title = {Double-Blind Reviewing at {{EvoLang}} 11 Reveals Gender Bias},
  author = {Roberts, Se{\'a}n G. and Verhoef, Tessa},
  year = {2016},
  month = jul,
  volume = {1},
  pages = {163--167},
  publisher = {{Oxford Academic}},
  issn = {2058-4571},
  doi = {10.1093/jole/lzw009},
  url = {https://academic.oup.com/jole/article/1/2/163/2281905},
  urldate = {2020-07-10},
  abstract = {Abstract.  The impact of introducing double-blind reviewing in the most recent Evolution of Language conference is assessed. The ranking of papers is compared b},
  journal = {Journal of Language Evolution},
  language = {en},
  number = {2}
}



@article{JoshiSantyEtAl_2020_State_and_Fate_of_Linguistic_Diversity_and_Inclusion_in_NLP_World,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  year = {2020},
  month = jun,
  url = {http://arxiv.org/abs/2004.09095},
  urldate = {2020-06-04},
  abstract = {Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the "language agnostic" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.},
  archivePrefix = {arXiv},
  eprint = {2004.09095},
  eprinttype = {arxiv},
  journal = {arXiv:2004.09095 [cs]},
  primaryClass = {cs}
}

@article{Mohammad_2020_Gender_Gap_in_Natural_Language_Processing_Research_Disparities_in_Authorship_and_Citations,
  title={Gender gap in natural language processing research: Disparities in authorship and citations},
  author={Mohammad, Saif M},
  journal={arXiv preprint arXiv:2005.00962},
  year={2020}
}


@article{PluchinoBiondoEtAl_2018_Talent_versus_luck_role_of_randomness_in_success_and_failure,
  title = {Talent versus Luck: The Role of Randomness in Success and Failure},
  shorttitle = {Talent versus Luck},
  author = {Pluchino, Alessandro and Biondo, Alessio Emanuele and Rapisarda, Andrea},
  year = {2018},
  month = may,
  volume = {21},
  pages = {1850014},
  issn = {0219-5259},
  doi = {10.1142/S0219525918500145},
  url = {https://www.worldscientific.com/doi/10.1142/S0219525918500145},
  urldate = {2019-10-28},
  abstract = {The largely dominant meritocratic paradigm of highly competitive Western cultures is rooted on the belief that success is mainly due, if not exclusively, to personal qualities such as talent, intelligence, skills, smartness, efforts, willfulness, hard work or risk taking. Sometimes, we are willing to admit that a certain degree of luck could also play a role in achieving significant success. But, as a matter of fact, it is rather common to underestimate the importance of external forces in individual successful stories. It is very well known that intelligence (or, more in general, talent and personal qualities) exhibits a Gaussian distribution among the population, whereas the distribution of wealth \textemdash{} often considered as a proxy of success \textemdash{} follows typically a power law (Pareto law), with a large majority of poor people and a very small number of billionaires. Such a discrepancy between a Normal distribution of inputs, with a typical scale (the average talent or intelligence), and the scale-invariant distribution of outputs, suggests that some hidden ingredient is at work behind the scenes. In this paper, we suggest that such an ingredient is just randomness. In particular, our simple agent-based model shows that, if it is true that some degree of talent is necessary to be successful in life, almost never the most talented people reach the highest peaks of success, being overtaken by averagely talented but sensibly luckier individuals. As far as we know, this counterintuitive result \textemdash{} although implicitly suggested between the lines in a vast literature \textemdash{} is quantified here for the first time. It sheds new light on the effectiveness of assessing merit on the basis of the reached level of success and underlines the risks of distributing excessive honors or resources to people who, at the end of the day, could have been simply luckier than others. We also compare several policy hypotheses to show the most efficient strategies for public funding of research, aiming to improve meritocracy, diversity of ideas and innovation.},
  journal = {Advances in Complex Systems},
  number = {03n04}
}


@article{TomkinsZhangEtAl_2017_Reviewer_bias_in_single-_versus_double-blind_peer_review,
  title = {Reviewer Bias in Single- versus Double-Blind Peer Review},
  author = {Tomkins, Andrew and Zhang, Min and Heavlin, William D.},
  year = {2017},
  month = nov,
  volume = {114},
  pages = {12708--12713},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1707323114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1707323114},
  urldate = {2020-07-13},
  abstract = {Peer review may be ``single-blind,'' in which reviewers are aware of the names and affiliations of paper authors, or ``double-blind,'' in which this information is hidden. Noting that computer science research often appears first or exclusively in peer-reviewed conferences rather than journals, we study these two reviewing models in the context of the 10th Association for Computing Machinery International Conference on Web Search and Data Mining, a highly selective venue (15.6\% acceptance rate) in which expert committee members review full-length submissions for acceptance. We present a controlled experiment in which four committee members review each paper. Two of these four reviewers are drawn from a pool of committee members with access to author information; the other two are drawn from a disjoint pool without such access. This information asymmetry persists through the process of bidding for papers, reviewing papers, and entering scores. Reviewers in the single-blind condition typically bid for 22\% fewer papers and preferentially bid for papers from top universities and companies. Once papers are allocated to reviewers, single-blind reviewers are significantly more likely than their double-blind counterparts to recommend for acceptance papers from famous authors, top universities, and top companies. The estimated odds multipliers are tangible, at 1.63, 1.58, and 2.10, respectively.},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}


@article{AlShebliRahwanEtAl_2018_preeminence_of_ethnic_diversity_in_scientific_collaboration,
  title = {The Preeminence of Ethnic Diversity in Scientific Collaboration},
  author = {AlShebli, Bedoor K. and Rahwan, Talal and Woon, Wei Lee},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {5163},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07634-8},
  url = {https://www.nature.com/articles/s41467-018-07634-8},
  urldate = {2020-07-13},
  abstract = {Inspired by the social and economic benefits of diversity, we analyze over 9 million papers and 6 million scientists to study the relationship between research impact and five classes of diversity: ethnicity, discipline, gender, affiliation, and academic age. Using randomized baseline models, we establish the presence of homophily in ethnicity, gender and affiliation. We then study the effect of diversity on scientific impact, as reflected in citations. Remarkably, of the classes considered, ethnic diversity had the strongest correlation with scientific impact. To further isolate the effects of ethnic diversity, we used randomized baseline models and again found a clear link between diversity and impact. To further support these findings, we use coarsened exact matching to compare the scientific impact of ethnically diverse papers and scientists with closely-matched control groups. Here, we find that ethnic diversity resulted in an impact gain of 10.63\% for papers, and 47.67\% for scientists.},
  copyright = {2018 The Author(s)},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}


@article{BornmannMutzEtAl_2007_Gender_differences_in_grant_peer_review_meta-analysis,
  title = {Gender Differences in Grant Peer Review: {{A}} Meta-Analysis},
  shorttitle = {Gender Differences in Grant Peer Review},
  author = {Bornmann, Lutz and Mutz, R{\"u}diger and Daniel, Hans-Dieter},
  year = {2007},
  month = jul,
  volume = {1},
  pages = {226--238},
  issn = {1751-1577},
  doi = {10.1016/j.joi.2007.03.001},
  url = {http://www.sciencedirect.com/science/article/pii/S1751157707000363},
  urldate = {2020-07-13},
  abstract = {Narrative reviews of peer review research have concluded that there is negligible evidence of gender bias in the awarding of grants based on peer review. Here, we report the findings of a meta-analysis of 21 studies providing, to the contrary, evidence of robust gender differences in grant award procedures. Even though the estimates of the gender effect vary substantially from study to study, the model estimation shows that all in all, among grant applicants men have statistically significant greater odds of receiving grants than women by about 7\%.},
  journal = {Journal of Informetrics},
  language = {en},
  number = {3},
  series = {The {{Hirsch Index}}}
}

@article{HojatGonnellaEtAl_2003_Impartial_Judgment_by_Gatekeepers_of_Science_Fallibility_and_Accountability_in_Peer_Review_Process,
  title = {Impartial {{Judgment}} by the ``{{Gatekeepers}}'' of {{Science}}: {{Fallibility}} and {{Accountability}} in the {{Peer Review Process}}},
  shorttitle = {Impartial {{Judgment}} by the ``{{Gatekeepers}}'' of {{Science}}},
  author = {Hojat, Mohammadreza and Gonnella, Joseph S. and Caelleigh, Addeane S.},
  year = {2003},
  month = mar,
  volume = {8},
  pages = {75--96},
  issn = {1573-1677},
  doi = {10.1023/A:1022670432373},
  url = {https://doi.org/10.1023/A:1022670432373},
  urldate = {2020-07-13},
  abstract = {High publication demands and the low acceptance rate of peer review journals place the journal editors and their reviewers in a powerful position. Journal reviewers have a vital role not only in influencing the journal editor's publication decisions, but also in the very nature and direction of scientific research. Because of their influence in peer review outcomes, journal reviewers are aptly described as the ``gate keepers of science.'' In this article we describe several pitfalls that can impede reviewers' impartial judgement. These include such issues as confirmatory bias, the negative results bias (the file drawer problem), the Matthew effect, the Doctor Fox effect, and gender, race, theoretical orientation, and ``political correctness.'' We argue that procedures currently used by many professional journals, such as blind or masked review, may not completely alleviate the effects of these pitfalls. Instead, we suggest that increasing reviewers' awareness of the pitfalls, accountability, and vigilance can improve fairness in the peer review process. The ultimate responsibilities belong to the journal editors who are confronted with the difficult task of satisfying journal readers, contributors, reviewers, and owners. We recommend that the journal editors conduct periodic internal and external evaluations of their journals' peer review process and outcomes, with participation of reviewers, contributors, readers and owners.},
  journal = {Advances in Health Sciences Education},
  language = {en},
  number = {1}
}

@article{KaatzGutierrezEtAl_2014_Threats_to_objectivity_in_peer_review_case_of_gender,
  title = {Threats to Objectivity in Peer Review: The Case of Gender},
  shorttitle = {Threats to Objectivity in Peer Review},
  author = {Kaatz, Anna and Gutierrez, Belinda and Carnes, Molly},
  year = {2014},
  month = aug,
  volume = {35},
  pages = {371--373},
  publisher = Elsevier,
  issn = {0165-6147},
  doi = {10.1016/j.tips.2014.06.005},
  url = {https://www.cell.com/trends/pharmacological-sciences/abstract/S0165-6147(14)00108-4},
  urldate = {2020-07-13},
  journal = {Trends in Pharmacological Sciences},
  language = {English},
  number = {8},
  pmid = {25086743}
}


@article{Merton_1968_Matthew_Effect_in_Science_reward_and_communication_systems_of_science_are_considered,
  title = {The {{Matthew Effect}} in {{Science}}: {{The}} Reward and Communication Systems of Science Are Considered},
  shorttitle = {The {{Matthew Effect}} in {{Science}}},
  author = {Merton, Robert K.},
  year = {1968},
  month = jan,
  volume = {159},
  pages = {56--63},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.159.3810.56},
  url = {https://science.sciencemag.org/content/159/3810/56},
  urldate = {2020-07-13},
  abstract = {This account of the Matthew effect is another small exercise in the psychosociological analysis of the workings of science as a social institution. The initial problem is transformed by a shift in theoretical perspective. As originally identified, the Matthew effect was construed in terms of enhancement of the position of already eminent scientists who are given disproportionate credit in cases of collaboration or of independent multiple discoveries. Its significance was thus confined to its implications for the reward system of science. By shifting the angle of vision, we note other possible kinds of consequences, this time for the communication system of science. The Matthew effect may serve to heighten the visibility of contributions to science by scientists of acknowledged standing and to reduce the visibility of contributions by authors who are less well known. We examine the psychosocial conditions and mechanisms underlying this effect and find a correlation between the redundancy function of multiple discoveries and the focalizing function of eminent men of science\textemdash a function which is reinforced by the great value these men place upon finding basic problems and by their self-assurance. This self-assurance, which is partly inherent, partly the result of experiences and associations in creative scientific environments, and partly a result of later social validation of their position, encourages them to search out risky but important problems and to highlight the results of their inquiry. A macrosocial version of the Matthew principle is apparently involved in those processes of social selection that currently lead to the concentration of scientific resources and talent (50).},
  chapter = {Articles},
  copyright = {\textcopyright{} 1968},
  journal = Science,
  language = {en},
  number = {3810},
  pmid = {5634379}
}


@article{Smith_2010_Classical_peer_review_empty_gun,
  title = {Classical Peer Review: An Empty Gun},
  shorttitle = {Classical Peer Review},
  author = {Smith, Richard},
  year = {2010},
  month = dec,
  volume = {12},
  pages = {S13},
  issn = {1465-542X},
  doi = {10.1186/bcr2742},
  url = {https://doi.org/10.1186/bcr2742},
  urldate = {2020-05-17},
  journal = {Breast Cancer Research},
  number = {4}
}


@article{Smith_2010_Classical_peer_review_empty_gun,
  title = {Classical Peer Review: An Empty Gun},
  shorttitle = {Classical Peer Review},
  author = {Smith, Richard},
  year = {2010},
  month = dec,
  volume = {12},
  pages = {S13},
  issn = {1465-542X},
  doi = {10.1186/bcr2742},
  url = {https://doi.org/10.1186/bcr2742},
  urldate = {2020-05-17},
  journal = {Breast Cancer Research},
  number = {4}
}













# On policy-making in NLP... on Twitter 

NLP is a very dynamic and fast-growing community, which also means it needs to be adapting quickly to the new realities. One of these realities is that the field is growing too fast to guarantee consistent, reliable peer review process. That increases the role of arXiv preprints in research discourse (if not in the eyes of the hiring committees).

We need to experiment to find new ways of doing things. And we are doing that, trying everything we can think of. Even for the selection of the reviewer pool, ACL and EMNLP chose completely different strategies:

* nominating all authors as potential reviewers in ACL 2020, with area chairs given choice to select people who would actually serve on PC;
* requiring a senior author to take on a full reviewing load in EMNLP 2020, either on their own or mentoring junior reviewers - under penalty of desk-rejected submissions;

Which strategy will yield better results? Unfortunately, time won't tell. The conferences cannot share the review data and particularly the data on rejected papers with the wider community, and there is no standard process they could use to analyze their respective data and then compare the results by the quality of reviews, rebuttals, and reviewer-author interaction. Since nobody knows which papers are rejected, we can't trace their further fate. 

Moreover, consider next year the conferences will be organized by completely different sets of people, who will have the same problems to solve, but little more than anecdotes and personal experience to base their decisions on. And the same thing will happen two, and three, and five, and ten years later. For researchers, we are being remarkably unscientific about the most important process that has so much impact on our careers.

# How policies are made

As a lowly postdoc, I have not yet organized any major conferences, so for this post I've reached out to some people who have. Here's what they say:

# Why is it so hard to make a good policy?

Policies in research communities are not quite as difficult as the US gun rights issue, or UK relationship with EU, but they are still tricky. As far as I can see, these are the fundamental issues:

 a) **Even goof ideas need to be well-formulated and sufficiently promoted to become a part of community mental toolkit.** <br/>
   *Case study*: consistent extension policy with regards to world events. After both EMNLP and NeurIPS 2020 extended their deadlines, there were (deservedly) cries of approval and support for #BlackLivesMatter, but also concern about much more support for US black researchers than for their colleagues in Mumbai whose submissions were likely affected by a catastrophic cyclone. Ideally, there would be a consistent policy shared by all conferences, that would be fair to researchers around the globe. But it won't happen without much work to come up with a good solution and to get all the conferences to incorporate it.
    
 b) **Potential different impact on different stakeholders**. <br/>
  *Case study*: anonymity period implemented by all *ACL conferences. While initially it was meant to level the playing field between well-known and obscure labs, the effect was largely to move the submission deadline a month earlier, and the well-known labs are less affected by it than the obscure labs, whose best chance would be to get the paper discussed and receive feedback. Something needs to be done about this situation, but whatever it is, it would need to carefully consider the likely effect on both groups. 
 
 c) **Mismatch between the academic ivory tower and the job market realities**. <br/>
   *Case study*: "Slow Science". An unquestionably good idea that came backed with the full authority of Yoshua Bengio, one of godfathers of AI. But even with the covid lockdown, which should by itself have slowed things down, EMNLP 2020 still received a record number of submissions again. The explanation is simple: while we would all love to write fewer, but better papers, our grant and hiring committees are still guided by the number of papers accepted to prestigious venues. Until that happens, we literally cannot afford to do good science.
   

Policies are hard because each contentious issue has cons and pros (often incomparable), and those cons and pros have different impact on different stakeholders (in our case, different groups of researchers), who would therefore be (dis)inclined to support them. The only way to move forward is to come to an agreement that X benefits the community.

# How we can make things better

Enter Twitter. 

This year I have the priviledge of being a publicity chair EMNLP and COLING. Usually that involves passing the information from the organizers to the community, with Twitter accounts serving as RSS feeds for the information posted on the website. Publicity chairs are not really the lead conference organizers, and do not have any decision-making power.

Now, publicity chairs don't have any power, but the community does. While nobody could tell a conference organizer to completely reverse their course of action, a sufficient number of complaints about a specific issue, a popular idea, a might just get through.

But there's something even more precious than that: the sheer power to think together. 

The remarkable thing that's been happening on Twitter is that when we're sharing our stories, we have a much better chance to formulate the new ideas clearly, to think through possible consequences, to run it past people who might not agree with it, and hear their side of the argument. Since it's asynchronous, we can write things up, present it to the powers that be, and come back to polish the idea so it's ready to be used. Imagine trying to condense all that in one ACL business meeting!

Here's a case study. In April 2020 I wrote a post about problems with reviewing resource papers, that sometimes get rejected *simply for being resource papers*. The post was based on extended discussions in [this](), [this](), and [this]() Twitter thread, with replies both from NLP stars like Yoav Goldberg and Emily Bender, and graduate students.

I wrote it up, and posted. [Got some more feedback]() from Mike Lewis, and addressed it. And then I tried to pitch it to EMNLP 2020 chairs, who were at that point working on their own reviewing policy post. I was told that the idea was good, but Mike's point needed to be made even clearer. I revised the post again... and now it's included in the official EMNLP 2020 reviewing advice.

Arguably, things already got better for the resource papers, since at least the awareness of the issue is raised significantly. But best of all, this story shows that community feedback is a powerful thing, and can help the organizers to improve more things than they themselves have the bandwidth for. 

It doesn't have to be Twitter, of course. As a matter of fact, Twitter is pretty horrible for threaded discussions, to the extent that an attempt to do an interview with Twitter's CEO on its own platform was a resounding fiasco. It's just that currently Twitter seems to have the most active #NLProc community out of LinkedIn, Reddit or Facebook (10% of COLING 2018 attendees used it).


## Scientific twittiwism



However, a social media platform with large representation of NLP community is also a great asynchronous tool for talking about things we, as a community, would like to be done better. Anybody, from any time zone and any remote institution can be heard. And that enables change in 

I started with my own pet peeve: conference peer review, which often yields  subjective, spurious paper evaluations. Turns out, I was not the only one dissatisfied with it. I was able to initiate many productive discussions with responses both from some of the top names in the field, such as Yoav Goldberg and Emily Bender, and not-yet-famous graduate students. 

Crucially, the results of these discussions could be distilled into blog posts, which could then be offered to the powers that be as concrete policy suggestions. In particular, I wrote about the [leaderboardism and the tendency to reject everything that is not SOTA](), and [the assumption of inferiority of resource papers](). 

I am proud to say that this worked, and the EMNLP organizers included this community effort in its official post on advice to reviewers:




But either way, we cannot only rely on time to tell which option yields better reviews. Peer review failures only become apparent in the rare cases of papers that are rejected, and then become well-cited in the future. For some, rejection from a major venue probably becomes a self-fulfilling prophecy, and the paper remains unknown. However, it is definitely the case that many accepted papers also remain unknown and unused.


Conference organizers only have so much time to analyze the outcomes of their chosen process, and they likely focus on their own specific questions. Some statistics and analysis is traditionally presented in the conference opening or closing speech, but not always made available for the whole community.
